[
    {
        "Function": "crypto_sha1_blockAMD64",
        "Total XOR and shift operations": 313,
        "XOR operations": 312,
        "Shift operations": 1,
        "Operation percentage": "44.97%",
        "Function Body": "\n// crypto/sha1.blockAMD64\n__int64 __golang crypto_sha1_blockAMD64(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        unsigned int *a10,\n        unsigned int *a11,\n        unsigned __int64 a12)\n{\n  unsigned int *v12; // rsi\n  unsigned __int64 v13; // rdi\n  __int64 result; // rax\n  unsigned int v15; // ebx\n  unsigned int v16; // ecx\n  unsigned int v17; // edx\n  unsigned int v18; // ebp\n  int v19; // r11d\n  unsigned int v20; // r12d\n  unsigned int v21; // r13d\n  unsigned int v22; // r14d\n  unsigned int v23; // r15d\n  int v24; // r9d\n  int v25; // ebx\n  int v26; // ebp\n  int v27; // r9d\n  int v28; // eax\n  int v29; // edx\n  int v30; // r9d\n  int v31; // ebp\n  int v32; // ecx\n  int v33; // r9d\n  int v34; // edx\n  int v35; // ebx\n  int v36; // r9d\n  int v37; // ecx\n  int v38; // eax\n  int v39; // r9d\n  int v40; // ebx\n  int v41; // ebp\n  int v42; // r9d\n  int v43; // eax\n  int v44; // edx\n  int v45; // r9d\n  int v46; // ebp\n  int v47; // ecx\n  int v48; // r9d\n  int v49; // edx\n  int v50; // ebx\n  int v51; // r9d\n  int v52; // ecx\n  int v53; // eax\n  int v54; // r9d\n  int v55; // ebx\n  int v56; // ebp\n  int v57; // r9d\n  int v58; // eax\n  int v59; // edx\n  int v60; // r9d\n  int v61; // ebp\n  int v62; // ecx\n  int v63; // r9d\n  int v64; // edx\n  int v65; // ebx\n  int v66; // r9d\n  int v67; // ecx\n  int v68; // eax\n  int v69; // r9d\n  int v70; // ebx\n  int v71; // ebp\n  int v72; // r9d\n  int v73; // eax\n  int v74; // edx\n  int v75; // r9d\n  int v76; // ebp\n  int v77; // ecx\n  int v78; // r9d\n  int v79; // edx\n  int v80; // ebx\n  int v81; // r9d\n  int v82; // ecx\n  int v83; // eax\n  int v84; // r9d\n  int v85; // ebx\n  int v86; // ebp\n  int v87; // r9d\n  int v88; // eax\n  int v89; // edx\n  int v90; // r9d\n  int v91; // ebp\n  int v92; // ecx\n  int v93; // r9d\n  int v94; // edx\n  int v95; // ebx\n  int v96; // r9d\n  int v97; // ecx\n  int v98; // eax\n  int v99; // r9d\n  int v100; // ebx\n  int v101; // ebp\n  int v102; // r9d\n  int v103; // eax\n  int v104; // edx\n  int v105; // r9d\n  int v106; // ebp\n  int v107; // ecx\n  int v108; // r9d\n  int v109; // edx\n  int v110; // ebx\n  int v111; // r9d\n  int v112; // ecx\n  int v113; // eax\n  int v114; // r9d\n  int v115; // ebx\n  int v116; // ebp\n  int v117; // r9d\n  int v118; // eax\n  int v119; // edx\n  int v120; // r9d\n  int v121; // ebp\n  int v122; // ecx\n  int v123; // r9d\n  int v124; // edx\n  int v125; // ebx\n  int v126; // r9d\n  int v127; // ecx\n  int v128; // eax\n  int v129; // r9d\n  int v130; // ebx\n  int v131; // ebp\n  int v132; // r9d\n  int v133; // eax\n  int v134; // edx\n  int v135; // r9d\n  int v136; // ebp\n  int v137; // ecx\n  int v138; // r9d\n  int v139; // edx\n  int v140; // ebx\n  int v141; // r9d\n  int v142; // ecx\n  int v143; // eax\n  int v144; // r9d\n  int v145; // ebx\n  int v146; // ebp\n  int v147; // r9d\n  int v148; // eax\n  int v149; // edx\n  int v150; // r9d\n  int v151; // ebp\n  int v152; // ecx\n  int v153; // r9d\n  int v154; // edx\n  int v155; // ebx\n  int v156; // r9d\n  int v157; // ecx\n  int v158; // eax\n  int v159; // r9d\n  int v160; // ebx\n  int v161; // ebp\n  int v162; // r9d\n  int v163; // eax\n  int v164; // edx\n  int v165; // r9d\n  int v166; // ebp\n  int v167; // ecx\n  int v168; // r9d\n  int v169; // edx\n  int v170; // ebx\n  int v171; // r9d\n  int v172; // ecx\n  int v173; // eax\n  int v174; // r9d\n  int v175; // ebx\n  int v176; // ebp\n  int v177; // r9d\n  int v178; // eax\n  int v179; // edx\n  int v180; // r9d\n  int v181; // ebp\n  int v182; // ecx\n  int v183; // r9d\n  int v184; // edx\n  int v185; // ebx\n  int v186; // r9d\n  int v187; // ecx\n  int v188; // eax\n  int v189; // r9d\n  int v190; // ebx\n  int v191; // ebp\n  int v192; // r9d\n  int v193; // eax\n  int v194; // edx\n  int v195; // r9d\n  int v196; // ebp\n  int v197; // ecx\n  int v198; // r9d\n  int v199; // edx\n  int v200; // ebx\n  int v201; // r9d\n  int v202; // ecx\n  int v203; // eax\n  int v204; // r9d\n  int v205; // ebx\n  int v206; // ebp\n  int v207; // r9d\n  int v208; // eax\n  int v209; // edx\n  int v210; // r9d\n  int v211; // ebp\n  int v212; // ecx\n  int v213; // r9d\n  int v214; // edx\n  int v215; // ebx\n  int v216; // r9d\n  int v217; // ecx\n  int v218; // eax\n  int v219; // r9d\n  int v220; // ebx\n  int v221; // ebp\n  int v222; // r9d\n  int v223; // eax\n  int v224; // edx\n  int v225; // r9d\n  int v226; // ebp\n  int v227; // ecx\n  int v228; // r9d\n  int v229; // edx\n  int v230; // ebx\n  int v231; // r9d\n  int v232; // ecx\n  int v233; // eax\n  int v234; // r9d\n  int v235; // ebx\n  int v236; // ebp\n  int v237; // r9d\n  int v238; // eax\n  int v239; // edx\n  int v240; // r9d\n  int v241; // ebp\n  int v242; // ecx\n  int v243; // r9d\n  int v244; // edx\n  int v245; // ebx\n  int v246; // r9d\n  int v247; // ecx\n  int v248; // eax\n  int v249; // r9d\n  int v250; // ebx\n  int v251; // ebp\n  int v252; // r9d\n  int v253; // eax\n  int v254; // edx\n  int v255; // r9d\n  int v256; // ebp\n  int v257; // ecx\n  int v258; // r9d\n  int v259; // edx\n  int v260; // ebx\n  unsigned __int32 v261; // [rsp+0h] [rbp-48h]\n  int v262; // [rsp+0h] [rbp-48h]\n  int v263; // [rsp+0h] [rbp-48h]\n  int v264; // [rsp+0h] [rbp-48h]\n  int v265; // [rsp+0h] [rbp-48h]\n  unsigned __int32 v266; // [rsp+4h] [rbp-44h]\n  int v267; // [rsp+4h] [rbp-44h]\n  int v268; // [rsp+4h] [rbp-44h]\n  int v269; // [rsp+4h] [rbp-44h]\n  int v270; // [rsp+4h] [rbp-44h]\n  unsigned __int32 v271; // [rsp+8h] [rbp-40h]\n  int v272; // [rsp+8h] [rbp-40h]\n  int v273; // [rsp+8h] [rbp-40h]\n  int v274; // [rsp+8h] [rbp-40h]\n  int v275; // [rsp+8h] [rbp-40h]\n  unsigned __int32 v276; // [rsp+Ch] [rbp-3Ch]\n  int v277; // [rsp+Ch] [rbp-3Ch]\n  int v278; // [rsp+Ch] [rbp-3Ch]\n  int v279; // [rsp+Ch] [rbp-3Ch]\n  int v280; // [rsp+Ch] [rbp-3Ch]\n  unsigned __int32 v281; // [rsp+10h] [rbp-38h]\n  int v282; // [rsp+10h] [rbp-38h]\n  int v283; // [rsp+10h] [rbp-38h]\n  int v284; // [rsp+10h] [rbp-38h]\n  int v285; // [rsp+10h] [rbp-38h]\n  unsigned __int32 v286; // [rsp+14h] [rbp-34h]\n  int v287; // [rsp+14h] [rbp-34h]\n  int v288; // [rsp+14h] [rbp-34h]\n  int v289; // [rsp+14h] [rbp-34h]\n  int v290; // [rsp+14h] [rbp-34h]\n  unsigned __int32 v291; // [rsp+18h] [rbp-30h]\n  int v292; // [rsp+18h] [rbp-30h]\n  int v293; // [rsp+18h] [rbp-30h]\n  int v294; // [rsp+18h] [rbp-30h]\n  int v295; // [rsp+18h] [rbp-30h]\n  unsigned __int32 v296; // [rsp+1Ch] [rbp-2Ch]\n  int v297; // [rsp+1Ch] [rbp-2Ch]\n  int v298; // [rsp+1Ch] [rbp-2Ch]\n  int v299; // [rsp+1Ch] [rbp-2Ch]\n  int v300; // [rsp+1Ch] [rbp-2Ch]\n  unsigned __int32 v301; // [rsp+20h] [rbp-28h]\n  int v302; // [rsp+20h] [rbp-28h]\n  int v303; // [rsp+20h] [rbp-28h]\n  int v304; // [rsp+20h] [rbp-28h]\n  int v305; // [rsp+20h] [rbp-28h]\n  unsigned __int32 v306; // [rsp+24h] [rbp-24h]\n  int v307; // [rsp+24h] [rbp-24h]\n  int v308; // [rsp+24h] [rbp-24h]\n  int v309; // [rsp+24h] [rbp-24h]\n  int v310; // [rsp+24h] [rbp-24h]\n  unsigned __int32 v311; // [rsp+28h] [rbp-20h]\n  int v312; // [rsp+28h] [rbp-20h]\n  int v313; // [rsp+28h] [rbp-20h]\n  int v314; // [rsp+28h] [rbp-20h]\n  int v315; // [rsp+28h] [rbp-20h]\n  unsigned __int32 v316; // [rsp+2Ch] [rbp-1Ch]\n  int v317; // [rsp+2Ch] [rbp-1Ch]\n  int v318; // [rsp+2Ch] [rbp-1Ch]\n  int v319; // [rsp+2Ch] [rbp-1Ch]\n  int v320; // [rsp+2Ch] [rbp-1Ch]\n  unsigned __int32 v321; // [rsp+30h] [rbp-18h]\n  int v322; // [rsp+30h] [rbp-18h]\n  int v323; // [rsp+30h] [rbp-18h]\n  int v324; // [rsp+30h] [rbp-18h]\n  int v325; // [rsp+30h] [rbp-18h]\n  unsigned __int32 v326; // [rsp+34h] [rbp-14h]\n  int v327; // [rsp+34h] [rbp-14h]\n  int v328; // [rsp+34h] [rbp-14h]\n  int v329; // [rsp+34h] [rbp-14h]\n  unsigned __int32 v330; // [rsp+38h] [rbp-10h]\n  int v331; // [rsp+38h] [rbp-10h]\n  int v332; // [rsp+38h] [rbp-10h]\n  int v333; // [rsp+38h] [rbp-10h]\n  unsigned __int32 v334; // [rsp+3Ch] [rbp-Ch]\n  int v335; // [rsp+3Ch] [rbp-Ch]\n  int v336; // [rsp+3Ch] [rbp-Ch]\n  int v337; // [rsp+3Ch] [rbp-Ch]\n\n  v12 = a11;\n  v13 = (unsigned __int64)&a11[16 * (a12 >> 6)];\n  result = *a10;\n  v15 = a10[1];\n  v16 = a10[2];\n  v17 = a10[3];\n  v18 = a10[4];\n  if ( a11 != (unsigned int *)v13 )\n  {\n    do\n    {\n      v19 = result;\n      v20 = v15;\n      v21 = v16;\n      v22 = v17;\n      v23 = v18;\n      v261 = _byteswap_ulong(*v12);\n      v24 = v17 ^ v15 & (v16 ^ v17);\n      v25 = __ROL4__(v15, 30);\n      v26 = __ROL4__(result, 5) + v24 + v18 + v261 + 1518500249;\n      v266 = _byteswap_ulong(v12[1]);\n      v27 = v16 ^ result & (v25 ^ v16);\n      v28 = __ROL4__(result, 30);\n      v29 = __ROL4__(v26, 5) + v27 + v17 + v266 + 1518500249;\n      v271 = _byteswap_ulong(v12[2]);\n      v30 = v25 ^ v26 & (v28 ^ v25);\n      v31 = __ROL4__(v26, 30);\n      v32 = __ROL4__(v29, 5) + v30 + v16 + v271 + 1518500249;\n      v276 = _byteswap_ulong(v12[3]);\n      v33 = v28 ^ v29 & (v31 ^ v28);\n      v34 = __ROL4__(v29, 30);\n      v35 = __ROL4__(v32, 5) + v33 + v25 + v276 + 1518500249;\n      v281 = _byteswap_ulong(v12[4]);\n      v36 = v31 ^ v32 & (v34 ^ v31);\n      v37 = __ROL4__(v32, 30);\n      v38 = __ROL4__(v35, 5) + v36 + v28 + v281 + 1518500249;\n      v286 = _byteswap_ulong(v12[5]);\n      v39 = v34 ^ v35 & (v37 ^ v34);\n      v40 = __ROL4__(v35, 30);\n      v41 = __ROL4__(v38, 5) + v39 + v31 + v286 + 1518500249;\n      v291 = _byteswap_ulong(v12[6]);\n      v42 = v37 ^ v38 & (v40 ^ v37);\n      v43 = __ROL4__(v38, 30);\n      v44 = __ROL4__(v41, 5) + v42 + v34 + v291 + 1518500249;\n      v296 = _byteswap_ulong(v12[7]);\n      v45 = v40 ^ v41 & (v43 ^ v40);\n      v46 = __ROL4__(v41, 30);\n      v47 = __ROL4__(v44, 5) + v45 + v37 + v296 + 1518500249;\n      v301 = _byteswap_ulong(v12[8]);\n      v48 = v43 ^ v44 & (v46 ^ v43);\n      v49 = __ROL4__(v44, 30);\n      v50 = __ROL4__(v47, 5) + v48 + v40 + v301 + 1518500249;\n      v306 = _byteswap_ulong(v12[9]);\n      v51 = v46 ^ v47 & (v49 ^ v46);\n      v52 = __ROL4__(v47, 30);\n      v53 = __ROL4__(v50, 5) + v51 + v43 + v306 + 1518500249;\n      v311 = _byteswap_ulong(v12[10]);\n      v54 = v49 ^ v50 & (v52 ^ v49);\n      v55 = __ROL4__(v50, 30);\n      v56 = __ROL4__(v53, 5) + v54 + v46 + v311 + 1518500249;\n      v316 = _byteswap_ulong(v12[11]);\n      v57 = v52 ^ v53 & (v55 ^ v52);\n      v58 = __ROL4__(v53, 30);\n      v59 = __ROL4__(v56, 5) + v57 + v49 + v316 + 1518500249;\n      v321 = _byteswap_ulong(v12[12]);\n      v60 = v55 ^ v56 & (v58 ^ v55);\n      v61 = __ROL4__(v56, 30);\n      v62 = __ROL4__(v59, 5) + v60 + v52 + v321 + 1518500249;\n      v326 = _byteswap_ulong(v12[13]);\n      v63 = v58 ^ v59 & (v61 ^ v58);\n      v64 = __ROL4__(v59, 30);\n      v65 = __ROL4__(v62, 5) + v63 + v55 + v326 + 1518500249;\n      v330 = _byteswap_ulong(v12[14]);\n      v66 = v61 ^ v62 & (v64 ^ v61);\n      v67 = __ROL4__(v62, 30);\n      v68 = __ROL4__(v65, 5) + v66 + v58 + v330 + 1518500249;\n      v334 = _byteswap_ulong(v12[15]);\n      v69 = v64 ^ v65 & (v67 ^ v64);\n      v70 = __ROL4__(v65, 30);\n      v71 = __ROL4__(v68, 5) + v69 + v61 + v334 + 1518500249;\n      v262 = __ROL4__(v271 ^ v301 ^ v326 ^ v261, 1);\n      v72 = v67 ^ v68 & (v70 ^ v67);\n      v73 = __ROL4__(v68, 30);\n      v74 = __ROL4__(v71, 5) + v72 + v64 + v262 + 1518500249;\n      v267 = __ROL4__(v276 ^ v306 ^ v330 ^ v266, 1);\n      v75 = v70 ^ v71 & (v73 ^ v70);\n      v76 = __ROL4__(v71, 30);\n      v77 = __ROL4__(v74, 5) + v75 + v67 + v267 + 1518500249;\n      v272 = __ROL4__(v281 ^ v311 ^ v334 ^ v271, 1);\n      v78 = v73 ^ v74 & (v76 ^ v73);\n      v79 = __ROL4__(v74, 30);\n      v80 = __ROL4__(v77, 5) + v78 + v70 + v272 + 1518500249;\n      v277 = __ROL4__(v286 ^ v316 ^ v262 ^ v276, 1);\n      v81 = v76 ^ v77 & (v79 ^ v76);\n      v82 = __ROL4__(v77, 30);\n      v83 = __ROL4__(v80, 5) + v81 + v73 + v277 + 1518500249;\n      v282 = __ROL4__(v291 ^ v321 ^ v267 ^ v281, 1);\n      v84 = v79 ^ v82 ^ v80;\n      v85 = __ROL4__(v80, 30);\n      v86 = __ROL4__(v83, 5) + v84 + v76 + v282 + 1859775393;\n      v287 = __ROL4__(v296 ^ v326 ^ v272 ^ v286, 1);\n      v87 = v82 ^ v85 ^ v83;\n      v88 = __ROL4__(v83, 30);\n      v89 = __ROL4__(v86, 5) + v87 + v79 + v287 + 1859775393;\n      v292 = __ROL4__(v301 ^ v330 ^ v277 ^ v291, 1);\n      v90 = v85 ^ v88 ^ v86;\n      v91 = __ROL4__(v86, 30);\n      v92 = __ROL4__(v89, 5) + v90 + v82 + v292 + 1859775393;\n      v297 = __ROL4__(v306 ^ v334 ^ v282 ^ v296, 1);\n      v93 = v88 ^ v91 ^ v89;\n      v94 = __ROL4__(v89, 30);\n      v95 = __ROL4__(v92, 5) + v93 + v85 + v297 + 1859775393;\n      v302 = __ROL4__(v311 ^ v262 ^ v287 ^ v301, 1);\n      v96 = v91 ^ v94 ^ v92;\n      v97 = __ROL4__(v92, 30);\n      v98 = __ROL4__(v95, 5) + v96 + v88 + v302 + 1859775393;\n      v307 = __ROL4__(v316 ^ v267 ^ v292 ^ v306, 1);\n      v99 = v94 ^ v97 ^ v95;\n      v100 = __ROL4__(v95, 30);\n      v101 = __ROL4__(v98, 5) + v99 + v91 + v307 + 1859775393;\n      v312 = __ROL4__(v321 ^ v272 ^ v297 ^ v311, 1);\n      v102 = v97 ^ v100 ^ v98;\n      v103 = __ROL4__(v98, 30);\n      v104 = __ROL4__(v101, 5) + v102 + v94 + v312 + 1859775393;\n      v317 = __ROL4__(v326 ^ v277 ^ v302 ^ v316, 1);\n      v105 = v100 ^ v103 ^ v101;\n      v106 = __ROL4__(v101, 30);\n      v107 = __ROL4__(v104, 5) + v105 + v97 + v317 + 1859775393;\n      v322 = __ROL4__(v330 ^ v282 ^ v307 ^ v321, 1);\n      v108 = v103 ^ v106 ^ v104;\n      v109 = __ROL4__(v104, 30);\n      v110 = __ROL4__(v107, 5) + v108 + v100 + v322 + 1859775393;\n      v327 = __ROL4__(v334 ^ v287 ^ v312 ^ v326, 1);\n      v111 = v106 ^ v109 ^ v107;\n      v112 = __ROL4__(v107, 30);\n      v113 = __ROL4__(v110, 5) + v111 + v103 + v327 + 1859775393;\n      v331 = __ROL4__(v262 ^ v292 ^ v317 ^ v330, 1);\n      v114 = v109 ^ v112 ^ v110;\n      v115 = __ROL4__(v110, 30);\n      v116 = __ROL4__(v113, 5) + v114 + v106 + v331 + 1859775393;\n      v335 = __ROL4__(v267 ^ v297 ^ v322 ^ v334, 1);\n      v117 = v112 ^ v115 ^ v113;\n      v118 = __ROL4__(v113, 30);\n      v119 = __ROL4__(v116, 5) + v117 + v109 + v335 + 1859775393;\n      v263 = __ROL4__(v272 ^ v302 ^ v327 ^ v262, 1);\n      v120 = v115 ^ v118 ^ v116;\n      v121 = __ROL4__(v116, 30);\n      v122 = __ROL4__(v119, 5) + v120 + v112 + v263 + 1859775393;\n      v268 = __ROL4__(v277 ^ v307 ^ v331 ^ v267, 1);\n      v123 = v118 ^ v121 ^ v119;\n      v124 = __ROL4__(v119, 30);\n      v125 = __ROL4__(v122, 5) + v123 + v115 + v268 + 1859775393;\n      v273 = __ROL4__(v282 ^ v312 ^ v335 ^ v272, 1);\n      v126 = v121 ^ v124 ^ v122;\n      v127 = __ROL4__(v122, 30);\n      v128 = __ROL4__(v125, 5) + v126 + v118 + v273 + 1859775393;\n      v278 = __ROL4__(v287 ^ v317 ^ v263 ^ v277, 1);\n      v129 = v124 ^ v127 ^ v125;\n      v130 = __ROL4__(v125, 30);\n      v131 = __ROL4__(v128, 5) + v129 + v121 + v278 + 1859775393;\n      v283 = __ROL4__(v292 ^ v322 ^ v268 ^ v282, 1);\n      v132 = v127 ^ v130 ^ v128;\n      v133 = __ROL4__(v128, 30);\n      v134 = __ROL4__(v131, 5) + v132 + v124 + v283 + 1859775393;\n      v288 = __ROL4__(v297 ^ v327 ^ v273 ^ v287, 1);\n      v135 = v130 ^ v133 ^ v131;\n      v136 = __ROL4__(v131, 30);\n      v137 = __ROL4__(v134, 5) + v135 + v127 + v288 + 1859775393;\n      v293 = __ROL4__(v302 ^ v331 ^ v278 ^ v292, 1);\n      v138 = v133 ^ v136 ^ v134;\n      v139 = __ROL4__(v134, 30);\n      v140 = __ROL4__(v137, 5) + v138 + v130 + v293 + 1859775393;\n      v298 = __ROL4__(v307 ^ v335 ^ v283 ^ v297, 1);\n      v141 = v136 ^ v139 ^ v137;\n      v142 = __ROL4__(v137, 30);\n      v143 = __ROL4__(v140, 5) + v141 + v133 + v298 + 1859775393;\n      v303 = __ROL4__(v312 ^ v263 ^ v288 ^ v302, 1);\n      v144 = v139 & (v142 | v140) | v142 & v140;\n      v145 = __ROL4__(v140, 30);\n      v146 = __ROL4__(v143, 5) + v144 + v136 + v303 - 1894007588;\n      v308 = __ROL4__(v317 ^ v268 ^ v293 ^ v307, 1);\n      v147 = v142 & (v145 | v143) | v145 & v143;\n      v148 = __ROL4__(v143, 30);\n      v149 = __ROL4__(v146, 5) + v147 + v139 + v308 - 1894007588;\n      v313 = __ROL4__(v322 ^ v273 ^ v298 ^ v312, 1);\n      v150 = v145 & (v148 | v146) | v148 & v146;\n      v151 = __ROL4__(v146, 30);\n      v152 = __ROL4__(v149, 5) + v150 + v142 + v313 - 1894007588;\n      v318 = __ROL4__(v327 ^ v278 ^ v303 ^ v317, 1);\n      v153 = v148 & (v151 | v149) | v151 & v149;\n      v154 = __ROL4__(v149, 30);\n      v155 = __ROL4__(v152, 5) + v153 + v145 + v318 - 1894007588;\n      v323 = __ROL4__(v331 ^ v283 ^ v308 ^ v322, 1);\n      v156 = v151 & (v154 | v152) | v154 & v152;\n      v157 = __ROL4__(v152, 30);\n      v158 = __ROL4__(v155, 5) + v156 + v148 + v323 - 1894007588;\n      v328 = __ROL4__(v335 ^ v288 ^ v313 ^ v327, 1);\n      v159 = v154 & (v157 | v155) | v157 & v155;\n      v160 = __ROL4__(v155, 30);\n      v161 = __ROL4__(v158, 5) + v159 + v151 + v328 - 1894007588;\n      v332 = __ROL4__(v263 ^ v293 ^ v318 ^ v331, 1);\n      v162 = v157 & (v160 | v158) | v160 & v158;\n      v163 = __ROL4__(v158, 30);\n      v164 = __ROL4__(v161, 5) + v162 + v154 + v332 - 1894007588;\n      v336 = __ROL4__(v268 ^ v298 ^ v323 ^ v335, 1);\n      v165 = v160 & (v163 | v161) | v163 & v161;\n      v166 = __ROL4__(v161, 30);\n      v167 = __ROL4__(v164, 5) + v165 + v157 + v336 - 1894007588;\n      v264 = __ROL4__(v273 ^ v303 ^ v328 ^ v263, 1);\n      v168 = v163 & (v166 | v164) | v166 & v164;\n      v169 = __ROL4__(v164, 30);\n      v170 = __ROL4__(v167, 5) + v168 + v160 + v264 - 1894007588;\n      v269 = __ROL4__(v278 ^ v308 ^ v332 ^ v268, 1);\n      v171 = v166 & (v169 | v167) | v169 & v167;\n      v172 = __ROL4__(v167, 30);\n      v173 = __ROL4__(v170, 5) + v171 + v163 + v269 - 1894007588;\n      v274 = __ROL4__(v283 ^ v313 ^ v336 ^ v273, 1);\n      v174 = v169 & (v172 | v170) | v172 & v170;\n      v175 = __ROL4__(v170, 30);\n      v176 = __ROL4__(v173, 5) + v174 + v166 + v274 - 1894007588;\n      v279 = __ROL4__(v288 ^ v318 ^ v264 ^ v278, 1);\n      v177 = v172 & (v175 | v173) | v175 & v173;\n      v178 = __ROL4__(v173, 30);\n      v179 = __ROL4__(v176, 5) + v177 + v169 + v279 - 1894007588;\n      v284 = __ROL4__(v293 ^ v323 ^ v269 ^ v283, 1);\n      v180 = v175 & (v178 | v176) | v178 & v176;\n      v181 = __ROL4__(v176, 30);\n      v182 = __ROL4__(v179, 5) + v180 + v172 + v284 - 1894007588;\n      v289 = __ROL4__(v298 ^ v328 ^ v274 ^ v288, 1);\n      v183 = v178 & (v181 | v179) | v181 & v179;\n      v184 = __ROL4__(v179, 30);\n      v185 = __ROL4__(v182, 5) + v183 + v175 + v289 - 1894007588;\n      v294 = __ROL4__(v303 ^ v332 ^ v279 ^ v293, 1);\n      v186 = v181 & (v184 | v182) | v184 & v182;\n      v187 = __ROL4__(v182, 30);\n      v188 = __ROL4__(v185, 5) + v186 + v178 + v294 - 1894007588;\n      v299 = __ROL4__(v308 ^ v336 ^ v284 ^ v298, 1);\n      v189 = v184 & (v187 | v185) | v187 & v185;\n      v190 = __ROL4__(v185, 30);\n      v191 = __ROL4__(v188, 5) + v189 + v181 + v299 - 1894007588;\n      v304 = __ROL4__(v313 ^ v264 ^ v289 ^ v303, 1);\n      v192 = v187 & (v190 | v188) | v190 & v188;\n      v193 = __ROL4__(v188, 30);\n      v194 = __ROL4__(v191, 5) + v192 + v184 + v304 - 1894007588;\n      v309 = __ROL4__(v318 ^ v269 ^ v294 ^ v308, 1);\n      v195 = v190 & (v193 | v191) | v193 & v191;\n      v196 = __ROL4__(v191, 30);\n      v197 = __ROL4__(v194, 5) + v195 + v187 + v309 - 1894007588;\n      v314 = __ROL4__(v323 ^ v274 ^ v299 ^ v313, 1);\n      v198 = v193 & (v196 | v194) | v196 & v194;\n      v199 = __ROL4__(v194, 30);\n      v200 = __ROL4__(v197, 5) + v198 + v190 + v314 - 1894007588;\n      v319 = __ROL4__(v328 ^ v279 ^ v304 ^ v318, 1);\n      v201 = v196 & (v199 | v197) | v199 & v197;\n      v202 = __ROL4__(v197, 30);\n      v203 = __ROL4__(v200, 5) + v201 + v193 + v319 - 1894007588;\n      v324 = __ROL4__(v332 ^ v284 ^ v309 ^ v323, 1);\n      v204 = v199 ^ v202 ^ v200;\n      v205 = __ROL4__(v200, 30);\n      v206 = __ROL4__(v203, 5) + v204 + v196 + v324 - 899497514;\n      v329 = __ROL4__(v336 ^ v289 ^ v314 ^ v328, 1);\n      v207 = v202 ^ v205 ^ v203;\n      v208 = __ROL4__(v203, 30);\n      v209 = __ROL4__(v206, 5) + v207 + v199 + v329 - 899497514;\n      v333 = __ROL4__(v264 ^ v294 ^ v319 ^ v332, 1);\n      v210 = v205 ^ v208 ^ v206;\n      v211 = __ROL4__(v206, 30);\n      v212 = __ROL4__(v209, 5) + v210 + v202 + v333 - 899497514;\n      v337 = __ROL4__(v269 ^ v299 ^ v324 ^ v336, 1);\n      v213 = v208 ^ v211 ^ v209;\n      v214 = __ROL4__(v209, 30);\n      v215 = __ROL4__(v212, 5) + v213 + v205 + v337 - 899497514;\n      v265 = __ROL4__(v274 ^ v304 ^ v329 ^ v264, 1);\n      v216 = v211 ^ v214 ^ v212;\n      v217 = __ROL4__(v212, 30);\n      v218 = __ROL4__(v215, 5) + v216 + v208 + v265 - 899497514;\n      v270 = __ROL4__(v279 ^ v309 ^ v333 ^ v269, 1);\n      v219 = v214 ^ v217 ^ v215;\n      v220 = __ROL4__(v215, 30);\n      v221 = __ROL4__(v218, 5) + v219 + v211 + v270 - 899497514;\n      v275 = __ROL4__(v284 ^ v314 ^ v337 ^ v274, 1);\n      v222 = v217 ^ v220 ^ v218;\n      v223 = __ROL4__(v218, 30);\n      v224 = __ROL4__(v221, 5) + v222 + v214 + v275 - 899497514;\n      v280 = __ROL4__(v289 ^ v319 ^ v265 ^ v279, 1);\n      v225 = v220 ^ v223 ^ v221;\n      v226 = __ROL4__(v221, 30);\n      v227 = __ROL4__(v224, 5) + v225 + v217 + v280 - 899497514;\n      v285 = __ROL4__(v294 ^ v324 ^ v270 ^ v284, 1);\n      v228 = v223 ^ v226 ^ v224;\n      v229 = __ROL4__(v224, 30);\n      v230 = __ROL4__(v227, 5) + v228 + v220 + v285 - 899497514;\n      v290 = __ROL4__(v299 ^ v329 ^ v275 ^ v289, 1);\n      v231 = v226 ^ v229 ^ v227;\n      v232 = __ROL4__(v227, 30);\n      v233 = __ROL4__(v230, 5) + v231 + v223 + v290 - 899497514;\n      v295 = __ROL4__(v304 ^ v333 ^ v280 ^ v294, 1);\n      v234 = v229 ^ v232 ^ v230;\n      v235 = __ROL4__(v230, 30);\n      v236 = __ROL4__(v233, 5) + v234 + v226 + v295 - 899497514;\n      v300 = __ROL4__(v309 ^ v337 ^ v285 ^ v299, 1);\n      v237 = v232 ^ v235 ^ v233;\n      v238 = __ROL4__(v233, 30);\n      v239 = __ROL4__(v236, 5) + v237 + v229 + v300 - 899497514;\n      v305 = __ROL4__(v314 ^ v265 ^ v290 ^ v304, 1);\n      v240 = v235 ^ v238 ^ v236;\n      v241 = __ROL4__(v236, 30);\n      v242 = __ROL4__(v239, 5) + v240 + v232 + v305 - 899497514;\n      v310 = __ROL4__(v319 ^ v270 ^ v295 ^ v309, 1);\n      v243 = v238 ^ v241 ^ v239;\n      v244 = __ROL4__(v239, 30);\n      v245 = __ROL4__(v242, 5) + v243 + v235 + v310 - 899497514;\n      v315 = __ROL4__(v324 ^ v275 ^ v300 ^ v314, 1);\n      v246 = v241 ^ v244 ^ v242;\n      v247 = __ROL4__(v242, 30);\n      v248 = __ROL4__(v245, 5) + v246 + v238 + v315 - 899497514;\n      v320 = __ROL4__(v329 ^ v280 ^ v305 ^ v319, 1);\n      v249 = v244 ^ v247 ^ v245;\n      v250 = __ROL4__(v245, 30);\n      v251 = __ROL4__(v248, 5) + v249 + v241 + v320 - 899497514;\n      v325 = __ROL4__(v333 ^ v285 ^ v310 ^ v324, 1);\n      v252 = v247 ^ v250 ^ v248;\n      v253 = __ROL4__(v248, 30);\n      v254 = __ROL4__(v251, 5) + v252 + v244 + v325 - 899497514;\n      v255 = v250 ^ v253 ^ v251;\n      v256 = __ROL4__(v251, 30);\n      v257 = __ROL4__(v254, 5) + v255 + v247 + __ROL4__(v337 ^ v290 ^ v315 ^ v329, 1) - 899497514;\n      v258 = v253 ^ v256 ^ v254;\n      v259 = __ROL4__(v254, 30);\n      v260 = __ROL4__(v257, 5) + v258 + v250 + __ROL4__(v265 ^ v295 ^ v320 ^ v333, 1) - 899497514;\n      result = v19\n             + __ROL4__(v260, 5)\n             + (v256 ^ v259 ^ (unsigned int)v257)\n             + v253\n             + __ROL4__(v270 ^ v300 ^ v325 ^ v337, 1)\n             - 899497514;\n      v15 = v20 + v260;\n      v16 = v21 + __ROL4__(v257, 30);\n      v17 = v22 + v259;\n      v18 = v23 + v256;\n      v12 += 16;\n    }\n    while ( (unsigned __int64)v12 < v13 );\n  }\n  *a10 = result;\n  a10[1] = v15;\n  a10[2] = v16;\n  a10[3] = v17;\n  a10[4] = v18;\n  return result;\n}\n\n"
    },
    {
        "Function": "crypto_md5_block",
        "Total XOR and shift operations": 81,
        "XOR operations": 80,
        "Shift operations": 1,
        "Operation percentage": "43.55%",
        "Function Body": "\n// crypto/md5.block\n__int64 __golang crypto_md5_block(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        unsigned int *a10,\n        _DWORD *a11,\n        unsigned __int64 a12)\n{\n  _DWORD *v12; // rsi\n  unsigned __int64 v13; // rdi\n  __int64 result; // rax\n  unsigned int v15; // ebx\n  unsigned int v16; // ecx\n  unsigned int v17; // edx\n  int v18; // r12d\n  unsigned int v19; // r13d\n  unsigned int v20; // r14d\n  unsigned int v21; // r15d\n  unsigned int v22; // eax\n  unsigned int v23; // edx\n  unsigned int v24; // ecx\n  unsigned int v25; // ebx\n  unsigned int v26; // eax\n  unsigned int v27; // edx\n  unsigned int v28; // ecx\n  unsigned int v29; // ebx\n  int v30; // eax\n  int v31; // edx\n  int v32; // ecx\n  int v33; // ebx\n  int v34; // eax\n  int v35; // edx\n  int v36; // ecx\n  int v37; // ebx\n  int v38; // eax\n  int v39; // edx\n  int v40; // ecx\n  int v41; // ebx\n  int v42; // eax\n  int v43; // edx\n  int v44; // ecx\n  int v45; // ebx\n  int v46; // eax\n  int v47; // edx\n  int v48; // ecx\n  int v49; // ebx\n  int v50; // eax\n  int v51; // edx\n  int v52; // ecx\n  int v53; // ebx\n  int v54; // eax\n  int v55; // edx\n  int v56; // ecx\n  int v57; // ebx\n  int v58; // eax\n  int v59; // edx\n  int v60; // ecx\n  int v61; // ebx\n  int v62; // eax\n  int v63; // edx\n  int v64; // ecx\n  int v65; // ebx\n  int v66; // eax\n  int v67; // edx\n  int v68; // ecx\n  int v69; // ebx\n  int v70; // eax\n  int v71; // edx\n  int v72; // ecx\n  int v73; // ebx\n  int v74; // eax\n  int v75; // edx\n  int v76; // ecx\n  int v77; // ebx\n  int v78; // eax\n  int v79; // edx\n  int v80; // ecx\n  int v81; // ebx\n  int v82; // eax\n  int v83; // edx\n  int v84; // ecx\n  int v85; // ebx\n\n  v12 = a11;\n  v13 = (unsigned __int64)&a11[16 * (a12 >> 6)];\n  result = *a10;\n  v15 = a10[1];\n  v16 = a10[2];\n  v17 = a10[3];\n  if ( a11 != (_DWORD *)v13 )\n  {\n    do\n    {\n      v18 = result;\n      v19 = v15;\n      v20 = v16;\n      v21 = v17;\n      v22 = v15 + __ROL4__((v17 ^ v15 & (v16 ^ v17)) + result + *v12 - 680876936, 7);\n      v23 = v22 + __ROL4__((v16 ^ v22 & (v15 ^ v16)) + v17 + v12[1] - 389564586, 12);\n      v24 = v23 + __ROL4__((v15 ^ v23 & (v22 ^ v15)) + v16 + v12[2] + 606105819, 17);\n      v25 = v24 + __ROL4__((v22 ^ v24 & (v23 ^ v22)) + v15 + v12[3] - 1044525330, 22);\n      v26 = v25 + __ROL4__((v23 ^ v25 & (v24 ^ v23)) + v22 + v12[4] - 176418897, 7);\n      v27 = v26 + __ROL4__((v24 ^ v26 & (v25 ^ v24)) + v23 + v12[5] + 1200080426, 12);\n      v28 = v27 + __ROL4__((v25 ^ v27 & (v26 ^ v25)) + v24 + v12[6] - 1473231341, 17);\n      v29 = v28 + __ROL4__((v26 ^ v28 & (v27 ^ v26)) + v25 + v12[7] - 45705983, 22);\n      v30 = v29 + __ROL4__((v27 ^ v29 & (v28 ^ v27)) + v26 + v12[8] + 1770035416, 7);\n      v31 = v30 + __ROL4__((v28 ^ v30 & (v29 ^ v28)) + v27 + v12[9] - 1958414417, 12);\n      v32 = v31 + __ROL4__((v29 ^ v31 & (v30 ^ v29)) + v28 + v12[10] - 42063, 17);\n      v33 = v32 + __ROL4__((v30 ^ v32 & (v31 ^ v30)) + v29 + v12[11] - 1990404162, 22);\n      v34 = v33 + __ROL4__((v31 ^ v33 & (v32 ^ v31)) + v30 + v12[12] + 1804603682, 7);\n      v35 = v34 + __ROL4__((v32 ^ v34 & (v33 ^ v32)) + v31 + v12[13] - 40341101, 12);\n      v36 = v35 + __ROL4__((v33 ^ v35 & (v34 ^ v33)) + v32 + v12[14] - 1502002290, 17);\n      v37 = v36 + __ROL4__((v34 ^ v36 & (v35 ^ v34)) + v33 + v12[15] + 1236535329, 22);\n      v38 = v37 + __ROL4__((v36 & ~v35 | v37 & v35) + v34 + v12[1] - 165796510, 5);\n      v39 = v38 + __ROL4__((v37 & ~v36 | v38 & v36) + v35 + v12[6] - 1069501632, 9);\n      v40 = v39 + __ROL4__((v38 & ~v37 | v39 & v37) + v36 + v12[11] + 643717713, 14);\n      v41 = v40 + __ROL4__((v39 & ~v38 | v40 & v38) + v37 + *v12 - 373897302, 20);\n      v42 = v41 + __ROL4__((v40 & ~v39 | v41 & v39) + v38 + v12[5] - 701558691, 5);\n      v43 = v42 + __ROL4__((v41 & ~v40 | v42 & v40) + v39 + v12[10] + 38016083, 9);\n      v44 = v43 + __ROL4__((v42 & ~v41 | v43 & v41) + v40 + v12[15] - 660478335, 14);\n      v45 = v44 + __ROL4__((v43 & ~v42 | v44 & v42) + v41 + v12[4] - 405537848, 20);\n      v46 = v45 + __ROL4__((v44 & ~v43 | v45 & v43) + v42 + v12[9] + 568446438, 5);\n      v47 = v46 + __ROL4__((v45 & ~v44 | v46 & v44) + v43 + v12[14] - 1019803690, 9);\n      v48 = v47 + __ROL4__((v46 & ~v45 | v47 & v45) + v44 + v12[3] - 187363961, 14);\n      v49 = v48 + __ROL4__((v47 & ~v46 | v48 & v46) + v45 + v12[8] + 1163531501, 20);\n      v50 = v49 + __ROL4__((v48 & ~v47 | v49 & v47) + v46 + v12[13] - 1444681467, 5);\n      v51 = v50 + __ROL4__((v49 & ~v48 | v50 & v48) + v47 + v12[2] - 51403784, 9);\n      v52 = v51 + __ROL4__((v50 & ~v49 | v51 & v49) + v48 + v12[7] + 1735328473, 14);\n      v53 = v52 + __ROL4__((v51 & ~v50 | v52 & v50) + v49 + v12[12] - 1926607734, 20);\n      v54 = v53 + __ROL4__((v53 ^ v51 ^ v52) + v50 + v12[5] - 378558, 4);\n      v55 = v54 + __ROL4__((v54 ^ v52 ^ v53) + v51 + v12[8] - 2022574463, 11);\n      v56 = v55 + __ROL4__((v55 ^ v53 ^ v54) + v52 + v12[11] + 1839030562, 16);\n      v57 = v56 + __ROL4__((v56 ^ v54 ^ v55) + v53 + v12[14] - 35309556, 23);\n      v58 = v57 + __ROL4__((v57 ^ v55 ^ v56) + v54 + v12[1] - 1530992060, 4);\n      v59 = v58 + __ROL4__((v58 ^ v56 ^ v57) + v55 + v12[4] + 1272893353, 11);\n      v60 = v59 + __ROL4__((v59 ^ v57 ^ v58) + v56 + v12[7] - 155497632, 16);\n      v61 = v60 + __ROL4__((v60 ^ v58 ^ v59) + v57 + v12[10] - 1094730640, 23);\n      v62 = v61 + __ROL4__((v61 ^ v59 ^ v60) + v58 + v12[13] + 681279174, 4);\n      v63 = v62 + __ROL4__((v62 ^ v60 ^ v61) + v59 + *v12 - 358537222, 11);\n      v64 = v63 + __ROL4__((v63 ^ v61 ^ v62) + v60 + v12[3] - 722521979, 16);\n      v65 = v64 + __ROL4__((v64 ^ v62 ^ v63) + v61 + v12[6] + 76029189, 23);\n      v66 = v65 + __ROL4__((v65 ^ v63 ^ v64) + v62 + v12[9] - 640364487, 4);\n      v67 = v66 + __ROL4__((v66 ^ v64 ^ v65) + v63 + v12[12] - 421815835, 11);\n      v68 = v67 + __ROL4__((v67 ^ v65 ^ v66) + v64 + v12[15] + 530742520, 16);\n      v69 = v68 + __ROL4__((v68 ^ v66 ^ v67) + v65 + v12[2] - 995338651, 23);\n      v70 = v69 + __ROL4__((v68 ^ (v69 | ~v67)) + v66 + *v12 - 198630844, 6);\n      v71 = v70 + __ROL4__((v69 ^ (v70 | ~v68)) + v67 + v12[7] + 1126891415, 10);\n      v72 = v71 + __ROL4__((v70 ^ (v71 | ~v69)) + v68 + v12[14] - 1416354905, 15);\n      v73 = v72 + __ROL4__((v71 ^ (v72 | ~v70)) + v69 + v12[5] - 57434055, 21);\n      v74 = v73 + __ROL4__((v72 ^ (v73 | ~v71)) + v70 + v12[12] + 1700485571, 6);\n      v75 = v74 + __ROL4__((v73 ^ (v74 | ~v72)) + v71 + v12[3] - 1894986606, 10);\n      v76 = v75 + __ROL4__((v74 ^ (v75 | ~v73)) + v72 + v12[10] - 1051523, 15);\n      v77 = v76 + __ROL4__((v75 ^ (v76 | ~v74)) + v73 + v12[1] - 2054922799, 21);\n      v78 = v77 + __ROL4__((v76 ^ (v77 | ~v75)) + v74 + v12[8] + 1873313359, 6);\n      v79 = v78 + __ROL4__((v77 ^ (v78 | ~v76)) + v75 + v12[15] - 30611744, 10);\n      v80 = v79 + __ROL4__((v78 ^ (v79 | ~v77)) + v76 + v12[6] - 1560198380, 15);\n      v81 = v80 + __ROL4__((v79 ^ (v80 | ~v78)) + v77 + v12[13] + 1309151649, 21);\n      v82 = v81 + __ROL4__((v80 ^ (v81 | ~v79)) + v78 + v12[4] - 145523070, 6);\n      v83 = v82 + __ROL4__((v81 ^ (v82 | ~v80)) + v79 + v12[11] - 1120210379, 10);\n      v84 = v83 + __ROL4__((v82 ^ (v83 | ~v81)) + v80 + v12[2] + 718787259, 15);\n      v85 = __ROL4__((v83 ^ (v84 | ~v82)) + v81 + v12[9] - 343485551, 21);\n      result = (unsigned int)(v18 + v82);\n      v15 = v19 + v84 + v85;\n      v16 = v20 + v84;\n      v17 = v21 + v83;\n      v12 += 16;\n    }\n    while ( (unsigned __int64)v12 < v13 );\n  }\n  *a10 = result;\n  a10[1] = v15;\n  a10[2] = v16;\n  a10[3] = v17;\n  return result;\n}\n// 4E4520: too many cbuild loops\n\n"
    },
    {
        "Function": "crypto_sha512_blockAVX2",
        "Total XOR and shift operations": 148,
        "XOR operations": 144,
        "Shift operations": 4,
        "Operation percentage": "26.24%",
        "Function Body": "\n// crypto/sha512.blockAVX2\nvoid __golang crypto_sha512_blockAVX2(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        __int64 *a10,\n        __int64 a11,\n        unsigned __int64 a12)\n{\n  __int64 v13; // rax\n  __int64 v14; // rbx\n  __int64 v15; // rcx\n  __int64 v16; // r8\n  __int64 v17; // rdx\n  __int64 v18; // r9\n  __int64 v19; // r10\n  __int64 v20; // r11\n  char *v22; // rbp\n  __int64 v41; // r11\n  __int64 v42; // r15\n  __int64 v43; // r8\n  __int64 v44; // r11\n  __int64 v55; // r10\n  __int64 v56; // r15\n  __int64 v57; // rcx\n  __int64 v58; // r10\n  __int64 v69; // r9\n  __int64 v70; // r15\n  __int64 v71; // rbx\n  __int64 v72; // r9\n  __int64 v82; // rdx\n  __int64 v83; // r15\n  __int64 v84; // rax\n  __int64 v85; // rdx\n  __int64 v96; // r8\n  __int64 v97; // r15\n  __int64 v98; // r11\n  __int64 v99; // r8\n  __int64 v110; // rcx\n  __int64 v111; // r15\n  __int64 v112; // r10\n  __int64 v113; // rcx\n  __int64 v124; // rbx\n  __int64 v125; // r15\n  __int64 v126; // r9\n  __int64 v127; // rbx\n  __int64 v137; // rax\n  __int64 v138; // r15\n  __int64 v139; // rdx\n  __int64 v140; // rax\n  __int64 v151; // r11\n  __int64 v152; // r15\n  __int64 v153; // r8\n  __int64 v154; // r11\n  __int64 v165; // r10\n  __int64 v166; // r15\n  __int64 v167; // rcx\n  __int64 v168; // r10\n  __int64 v179; // r9\n  __int64 v180; // r15\n  __int64 v181; // rbx\n  __int64 v182; // r9\n  __int64 v192; // rdx\n  __int64 v193; // r15\n  __int64 v194; // rax\n  __int64 v195; // rdx\n  __int64 v206; // r8\n  __int64 v207; // r15\n  __int64 v218; // rcx\n  __int64 v219; // r15\n  __int64 v230; // rbx\n  __int64 v231; // r15\n  __int64 v241; // rax\n  __int64 v242; // r15\n  __int64 v244; // r11\n  __int64 v245; // r15\n  __int64 v246; // r8\n  __int64 v247; // r11\n  __int64 v248; // r10\n  __int64 v249; // r15\n  __int64 v250; // rcx\n  __int64 v251; // r10\n  __int64 v252; // r9\n  __int64 v253; // r15\n  __int64 v254; // rbx\n  __int64 v255; // r9\n  __int64 v256; // rdx\n  __int64 v257; // r15\n  __int64 v258; // rax\n  __int64 v259; // rdx\n  __int64 v261; // r8\n  __int64 v262; // r15\n  __int64 v263; // rcx\n  __int64 v264; // r15\n  __int64 v265; // rbx\n  __int64 v266; // r15\n  __int64 v267; // rax\n  __int64 v268; // r15\n  __m256i v269; // [rsp+0h] [rbp-40h]\n  __m256i v270; // [rsp+0h] [rbp-40h]\n  __m256i v271; // [rsp+0h] [rbp-40h]\n  __m256i v272; // [rsp+0h] [rbp-40h]\n  __m256i v273; // [rsp+0h] [rbp-40h]\n  __m256i v274; // [rsp+0h] [rbp-40h]\n  __int64 v275; // [rsp+20h] [rbp-20h]\n  __int64 v276; // [rsp+20h] [rbp-20h]\n  __int64 v277; // [rsp+28h] [rbp-18h]\n\n  _RDI = a11;\n  if ( a12 >> 7 << 7 )\n  {\n    v13 = *a10;\n    v14 = a10[1];\n    v15 = a10[2];\n    v16 = a10[3];\n    v17 = a10[4];\n    v18 = a10[5];\n    v19 = a10[6];\n    v20 = a10[7];\n    __asm { vmovdqu ymm9, cs:ymmword_5493A0 }\n    do\n    {\n      v22 = (char *)off_5EEC60;\n      __asm\n      {\n        vmovdqu ymm4, ymmword ptr [rdi]\n        vpshufb ymm4, ymm4, ymm9\n        vmovdqu ymm5, ymmword ptr [rdi+20h]\n        vpshufb ymm5, ymm5, ymm9\n        vmovdqu ymm6, ymmword ptr [rdi+40h]\n        vpshufb ymm6, ymm6, ymm9\n        vmovdqu ymm7, ymmword ptr [rdi+60h]\n        vpshufb ymm7, ymm7, ymm9\n      }\n      v277 = _RDI;\n      v275 = 4LL;\n      do\n      {\n        __asm\n        {\n          vpaddq  ymm0, ymm4, ymmword ptr [rbp+0]\n          vmovdqu [rsp+40h+var_40], ymm0\n          vperm2f128 ymm0, ymm7, ymm6, 3\n          vpalignr ymm0, ymm0, ymm6, 8\n          vpaddq  ymm0, ymm0, ymm4\n          vperm2f128 ymm1, ymm5, ymm4, 3\n          vpalignr ymm1, ymm1, ymm4, 8\n          vpsrlq  ymm2, ymm1, 1\n          vpsllq  ymm3, ymm1, 3Fh ; '?'\n          vpor    ymm3, ymm3, ymm2\n          vpsrlq  ymm8, ymm1, 7\n        }\n        v41 = v269.m256i_i64[0] + v20;\n        v42 = (__ROR8__(v17, 14) ^ __ROR8__(v17, 18) ^ __ROR8__(v17, 41)) + (v19 ^ v17 & (v19 ^ v18));\n        v43 = v42 + v41 + v16;\n        v44 = (v15 & v13 | v14 & (v15 | v13)) + v42 + (__ROR8__(v13, 28) ^ __ROR8__(v13, 34) ^ __ROR8__(v13, 39)) + v41;\n        __asm\n        {\n          vpsrlq  ymm2, ymm1, 8\n          vpsllq  ymm1, ymm1, 38h ; '8'\n          vpor    ymm1, ymm1, ymm2\n          vpxor   ymm3, ymm3, ymm8\n          vpxor   ymm1, ymm3, ymm1\n          vpaddq  ymm0, ymm0, ymm1\n          vperm2f128 ymm4, ymm0, ymm0, 0\n          vpand   ymm0, ymm0, cs:ymmword_549380\n          vperm2f128 ymm2, ymm7, ymm7, 11h\n          vpsrlq  ymm8, ymm2, 6\n        }\n        v55 = v269.m256i_i64[1] + v19;\n        v56 = (__ROR8__(v43, 14) ^ __ROR8__(v43, 18) ^ __ROR8__(v43, 41)) + (v18 ^ v43 & (v18 ^ v17));\n        v57 = v56 + v55 + v15;\n        v58 = (v14 & v44 | v13 & (v14 | v44)) + v56 + (__ROR8__(v44, 28) ^ __ROR8__(v44, 34) ^ __ROR8__(v44, 39)) + v55;\n        __asm\n        {\n          vpsrlq  ymm3, ymm2, 13h\n          vpsllq  ymm1, ymm2, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm2, 3Dh ; '='\n          vpsllq  ymm1, ymm2, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm4, ymm4, ymm8\n          vpsrlq  ymm8, ymm4, 6\n        }\n        v69 = v269.m256i_i64[2] + v18;\n        v70 = (__ROR8__(v57, 14) ^ __ROR8__(v57, 18) ^ __ROR8__(v57, 41)) + (v17 ^ v57 & (v17 ^ v43));\n        v71 = v70 + v69 + v14;\n        v72 = (v13 & v58 | v44 & (v13 | v58)) + v70 + (__ROR8__(v58, 28) ^ __ROR8__(v58, 34) ^ __ROR8__(v58, 39)) + v69;\n        __asm\n        {\n          vpsrlq  ymm3, ymm4, 13h\n          vpsllq  ymm1, ymm4, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm4, 3Dh ; '='\n          vpsllq  ymm1, ymm4, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm2, ymm0, ymm8\n          vpblendd ymm4, ymm4, ymm2, 0F0h\n        }\n        v82 = v269.m256i_i64[3] + v17;\n        v83 = (__ROR8__(v71, 14) ^ __ROR8__(v71, 18) ^ __ROR8__(v71, 41)) + (v43 ^ v71 & (v43 ^ v57));\n        v84 = v83 + v82 + v13;\n        v85 = (v44 & v72 | v58 & (v44 | v72)) + v83 + (__ROR8__(v72, 28) ^ __ROR8__(v72, 34) ^ __ROR8__(v72, 39)) + v82;\n        __asm\n        {\n          vpaddq  ymm0, ymm5, ymmword ptr [rbp+20h]\n          vmovdqu [rsp+40h+var_40], ymm0\n          vperm2f128 ymm0, ymm4, ymm7, 3\n          vpalignr ymm0, ymm0, ymm7, 8\n          vpaddq  ymm0, ymm0, ymm5\n          vperm2f128 ymm1, ymm6, ymm5, 3\n          vpalignr ymm1, ymm1, ymm5, 8\n          vpsrlq  ymm2, ymm1, 1\n          vpsllq  ymm3, ymm1, 3Fh ; '?'\n          vpor    ymm3, ymm3, ymm2\n          vpsrlq  ymm8, ymm1, 7\n        }\n        v96 = v270.m256i_i64[0] + v43;\n        v97 = (__ROR8__(v84, 14) ^ __ROR8__(v84, 18) ^ __ROR8__(v84, 41)) + (v57 ^ v84 & (v57 ^ v71));\n        v98 = v97 + v96 + v44;\n        v99 = (v58 & v85 | v72 & (v58 | v85)) + v97 + (__ROR8__(v85, 28) ^ __ROR8__(v85, 34) ^ __ROR8__(v85, 39)) + v96;\n        __asm\n        {\n          vpsrlq  ymm2, ymm1, 8\n          vpsllq  ymm1, ymm1, 38h ; '8'\n          vpor    ymm1, ymm1, ymm2\n          vpxor   ymm3, ymm3, ymm8\n          vpxor   ymm1, ymm3, ymm1\n          vpaddq  ymm0, ymm0, ymm1\n          vperm2f128 ymm5, ymm0, ymm0, 0\n          vpand   ymm0, ymm0, cs:ymmword_549380\n          vperm2f128 ymm2, ymm4, ymm4, 11h\n          vpsrlq  ymm8, ymm2, 6\n        }\n        v110 = v270.m256i_i64[1] + v57;\n        v111 = (__ROR8__(v98, 14) ^ __ROR8__(v98, 18) ^ __ROR8__(v98, 41)) + (v71 ^ v98 & (v71 ^ v84));\n        v112 = v111 + v110 + v58;\n        v113 = (v72 & v99 | v85 & (v72 | v99))\n             + v111\n             + (__ROR8__(v99, 28) ^ __ROR8__(v99, 34) ^ __ROR8__(v99, 39))\n             + v110;\n        __asm\n        {\n          vpsrlq  ymm3, ymm2, 13h\n          vpsllq  ymm1, ymm2, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm2, 3Dh ; '='\n          vpsllq  ymm1, ymm2, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm5, ymm5, ymm8\n          vpsrlq  ymm8, ymm5, 6\n        }\n        v124 = v270.m256i_i64[2] + v71;\n        v125 = (__ROR8__(v112, 14) ^ __ROR8__(v112, 18) ^ __ROR8__(v112, 41)) + (v84 ^ v112 & (v84 ^ v98));\n        v126 = v125 + v124 + v72;\n        v127 = (v85 & v113 | v99 & (v85 | v113))\n             + v125\n             + (__ROR8__(v113, 28) ^ __ROR8__(v113, 34) ^ __ROR8__(v113, 39))\n             + v124;\n        __asm\n        {\n          vpsrlq  ymm3, ymm5, 13h\n          vpsllq  ymm1, ymm5, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm5, 3Dh ; '='\n          vpsllq  ymm1, ymm5, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm2, ymm0, ymm8\n          vpblendd ymm5, ymm5, ymm2, 0F0h\n        }\n        v137 = v270.m256i_i64[3] + v84;\n        v138 = (__ROR8__(v126, 14) ^ __ROR8__(v126, 18) ^ __ROR8__(v126, 41)) + (v98 ^ v126 & (v98 ^ v112));\n        v139 = v138 + v137 + v85;\n        v140 = (v99 & v127 | v113 & (v99 | v127))\n             + v138\n             + (__ROR8__(v127, 28) ^ __ROR8__(v127, 34) ^ __ROR8__(v127, 39))\n             + v137;\n        __asm\n        {\n          vpaddq  ymm0, ymm6, ymmword ptr [rbp+40h]\n          vmovdqu [rsp+40h+var_40], ymm0\n          vperm2f128 ymm0, ymm5, ymm4, 3\n          vpalignr ymm0, ymm0, ymm4, 8\n          vpaddq  ymm0, ymm0, ymm6\n          vperm2f128 ymm1, ymm7, ymm6, 3\n          vpalignr ymm1, ymm1, ymm6, 8\n          vpsrlq  ymm2, ymm1, 1\n          vpsllq  ymm3, ymm1, 3Fh ; '?'\n          vpor    ymm3, ymm3, ymm2\n          vpsrlq  ymm8, ymm1, 7\n        }\n        v151 = v271.m256i_i64[0] + v98;\n        v152 = (__ROR8__(v139, 14) ^ __ROR8__(v139, 18) ^ __ROR8__(v139, 41)) + (v112 ^ v139 & (v112 ^ v126));\n        v153 = v152 + v151 + v99;\n        v154 = (v113 & v140 | v127 & (v113 | v140))\n             + v152\n             + (__ROR8__(v140, 28) ^ __ROR8__(v140, 34) ^ __ROR8__(v140, 39))\n             + v151;\n        __asm\n        {\n          vpsrlq  ymm2, ymm1, 8\n          vpsllq  ymm1, ymm1, 38h ; '8'\n          vpor    ymm1, ymm1, ymm2\n          vpxor   ymm3, ymm3, ymm8\n          vpxor   ymm1, ymm3, ymm1\n          vpaddq  ymm0, ymm0, ymm1\n          vperm2f128 ymm6, ymm0, ymm0, 0\n          vpand   ymm0, ymm0, cs:ymmword_549380\n          vperm2f128 ymm2, ymm5, ymm5, 11h\n          vpsrlq  ymm8, ymm2, 6\n        }\n        v165 = v271.m256i_i64[1] + v112;\n        v166 = (__ROR8__(v153, 14) ^ __ROR8__(v153, 18) ^ __ROR8__(v153, 41)) + (v126 ^ v153 & (v126 ^ v139));\n        v167 = v166 + v165 + v113;\n        v168 = (v127 & v154 | v140 & (v127 | v154))\n             + v166\n             + (__ROR8__(v154, 28) ^ __ROR8__(v154, 34) ^ __ROR8__(v154, 39))\n             + v165;\n        __asm\n        {\n          vpsrlq  ymm3, ymm2, 13h\n          vpsllq  ymm1, ymm2, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm2, 3Dh ; '='\n          vpsllq  ymm1, ymm2, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm6, ymm6, ymm8\n          vpsrlq  ymm8, ymm6, 6\n        }\n        v179 = v271.m256i_i64[2] + v126;\n        v180 = (__ROR8__(v167, 14) ^ __ROR8__(v167, 18) ^ __ROR8__(v167, 41)) + (v139 ^ v167 & (v139 ^ v153));\n        v181 = v180 + v179 + v127;\n        v182 = (v140 & v168 | v154 & (v140 | v168))\n             + v180\n             + (__ROR8__(v168, 28) ^ __ROR8__(v168, 34) ^ __ROR8__(v168, 39))\n             + v179;\n        __asm\n        {\n          vpsrlq  ymm3, ymm6, 13h\n          vpsllq  ymm1, ymm6, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm6, 3Dh ; '='\n          vpsllq  ymm1, ymm6, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm2, ymm0, ymm8\n          vpblendd ymm6, ymm6, ymm2, 0F0h\n        }\n        v192 = v271.m256i_i64[3] + v139;\n        v193 = (__ROR8__(v181, 14) ^ __ROR8__(v181, 18) ^ __ROR8__(v181, 41)) + (v153 ^ v181 & (v153 ^ v167));\n        v194 = v193 + v192 + v140;\n        v195 = (v154 & v182 | v168 & (v154 | v182))\n             + v193\n             + (__ROR8__(v182, 28) ^ __ROR8__(v182, 34) ^ __ROR8__(v182, 39))\n             + v192;\n        __asm\n        {\n          vpaddq  ymm0, ymm7, ymmword ptr [rbp+60h]\n          vmovdqu [rsp+40h+var_40], ymm0\n        }\n        v22 += 128;\n        __asm\n        {\n          vperm2f128 ymm0, ymm6, ymm5, 3\n          vpalignr ymm0, ymm0, ymm5, 8\n          vpaddq  ymm0, ymm0, ymm7\n          vperm2f128 ymm1, ymm4, ymm7, 3\n          vpalignr ymm1, ymm1, ymm7, 8\n          vpsrlq  ymm2, ymm1, 1\n          vpsllq  ymm3, ymm1, 3Fh ; '?'\n          vpor    ymm3, ymm3, ymm2\n          vpsrlq  ymm8, ymm1, 7\n        }\n        v206 = v272.m256i_i64[0] + v153;\n        v207 = (__ROR8__(v194, 14) ^ __ROR8__(v194, 18) ^ __ROR8__(v194, 41)) + (v167 ^ v194 & (v167 ^ v181));\n        v20 = v207 + v206 + v154;\n        v16 = (v168 & v195 | v182 & (v168 | v195))\n            + v207\n            + (__ROR8__(v195, 28) ^ __ROR8__(v195, 34) ^ __ROR8__(v195, 39))\n            + v206;\n        __asm\n        {\n          vpsrlq  ymm2, ymm1, 8\n          vpsllq  ymm1, ymm1, 38h ; '8'\n          vpor    ymm1, ymm1, ymm2\n          vpxor   ymm3, ymm3, ymm8\n          vpxor   ymm1, ymm3, ymm1\n          vpaddq  ymm0, ymm0, ymm1\n          vperm2f128 ymm7, ymm0, ymm0, 0\n          vpand   ymm0, ymm0, cs:ymmword_549380\n          vperm2f128 ymm2, ymm6, ymm6, 11h\n          vpsrlq  ymm8, ymm2, 6\n        }\n        v218 = v272.m256i_i64[1] + v167;\n        v219 = (__ROR8__(v20, 14) ^ __ROR8__(v20, 18) ^ __ROR8__(v20, 41)) + (v181 ^ v20 & (v181 ^ v194));\n        v19 = v219 + v218 + v168;\n        v15 = (v182 & v16 | v195 & (v182 | v16))\n            + v219\n            + (__ROR8__(v16, 28) ^ __ROR8__(v16, 34) ^ __ROR8__(v16, 39))\n            + v218;\n        __asm\n        {\n          vpsrlq  ymm3, ymm2, 13h\n          vpsllq  ymm1, ymm2, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm2, 3Dh ; '='\n          vpsllq  ymm1, ymm2, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm7, ymm7, ymm8\n          vpsrlq  ymm8, ymm7, 6\n        }\n        v230 = v272.m256i_i64[2] + v181;\n        v231 = (__ROR8__(v19, 14) ^ __ROR8__(v19, 18) ^ __ROR8__(v19, 41)) + (v194 ^ v19 & (v194 ^ v20));\n        v18 = v231 + v230 + v182;\n        v14 = (v195 & v15 | v16 & (v195 | v15))\n            + v231\n            + (__ROR8__(v15, 28) ^ __ROR8__(v15, 34) ^ __ROR8__(v15, 39))\n            + v230;\n        __asm\n        {\n          vpsrlq  ymm3, ymm7, 13h\n          vpsllq  ymm1, ymm7, 2Dh ; '-'\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpsrlq  ymm3, ymm7, 3Dh ; '='\n          vpsllq  ymm1, ymm7, 3\n          vpor    ymm3, ymm3, ymm1\n          vpxor   ymm8, ymm8, ymm3\n          vpaddq  ymm2, ymm0, ymm8\n          vpblendd ymm7, ymm7, ymm2, 0F0h\n        }\n        v241 = v272.m256i_i64[3] + v194;\n        v242 = (__ROR8__(v18, 14) ^ __ROR8__(v18, 18) ^ __ROR8__(v18, 41)) + (v20 ^ v18 & (v20 ^ v19));\n        v17 = v242 + v241 + v195;\n        v13 = (v16 & v14 | v15 & (v16 | v14))\n            + v242\n            + (__ROR8__(v14, 28) ^ __ROR8__(v14, 34) ^ __ROR8__(v14, 39))\n            + v241;\n        --v275;\n      }\n      while ( v275 );\n      v276 = 2LL;\n      do\n      {\n        __asm\n        {\n          vpaddq  ymm0, ymm4, ymmword ptr [rbp+0]\n          vmovdqu [rsp+40h+var_40], ymm0\n        }\n        v244 = v273.m256i_i64[0] + v20;\n        v245 = (__ROR8__(v17, 14) ^ __ROR8__(v17, 18) ^ __ROR8__(v17, 41)) + (v19 ^ v17 & (v19 ^ v18));\n        v246 = v245 + v244 + v16;\n        v247 = (v15 & v13 | v14 & (v15 | v13))\n             + v245\n             + (__ROR8__(v13, 28) ^ __ROR8__(v13, 34) ^ __ROR8__(v13, 39))\n             + v244;\n        v248 = v273.m256i_i64[1] + v19;\n        v249 = (__ROR8__(v246, 14) ^ __ROR8__(v246, 18) ^ __ROR8__(v246, 41)) + (v18 ^ v246 & (v18 ^ v17));\n        v250 = v249 + v248 + v15;\n        v251 = (v14 & v247 | v13 & (v14 | v247))\n             + v249\n             + (__ROR8__(v247, 28) ^ __ROR8__(v247, 34) ^ __ROR8__(v247, 39))\n             + v248;\n        v252 = v273.m256i_i64[2] + v18;\n        v253 = (__ROR8__(v250, 14) ^ __ROR8__(v250, 18) ^ __ROR8__(v250, 41)) + (v17 ^ v250 & (v17 ^ v246));\n        v254 = v253 + v252 + v14;\n        v255 = (v13 & v251 | v247 & (v13 | v251))\n             + v253\n             + (__ROR8__(v251, 28) ^ __ROR8__(v251, 34) ^ __ROR8__(v251, 39))\n             + v252;\n        v256 = v273.m256i_i64[3] + v17;\n        v257 = (__ROR8__(v254, 14) ^ __ROR8__(v254, 18) ^ __ROR8__(v254, 41)) + (v246 ^ v254 & (v246 ^ v250));\n        v258 = v257 + v256 + v13;\n        v259 = (v247 & v255 | v251 & (v247 | v255))\n             + v257\n             + (__ROR8__(v255, 28) ^ __ROR8__(v255, 34) ^ __ROR8__(v255, 39))\n             + v256;\n        __asm\n        {\n          vpaddq  ymm0, ymm5, ymmword ptr [rbp+20h]\n          vmovdqu [rsp+40h+var_40], ymm0\n        }\n        v22 += 64;\n        v261 = v274.m256i_i64[0] + v246;\n        v262 = (__ROR8__(v258, 14) ^ __ROR8__(v258, 18) ^ __ROR8__(v258, 41)) + (v250 ^ v258 & (v250 ^ v254));\n        v20 = v262 + v261 + v247;\n        v16 = (v251 & v259 | v255 & (v251 | v259))\n            + v262\n            + (__ROR8__(v259, 28) ^ __ROR8__(v259, 34) ^ __ROR8__(v259, 39))\n            + v261;\n        v263 = v274.m256i_i64[1] + v250;\n        v264 = (__ROR8__(v20, 14) ^ __ROR8__(v20, 18) ^ __ROR8__(v20, 41)) + (v254 ^ v20 & (v254 ^ v258));\n        v19 = v264 + v263 + v251;\n        v15 = (v255 & v16 | v259 & (v255 | v16))\n            + v264\n            + (__ROR8__(v16, 28) ^ __ROR8__(v16, 34) ^ __ROR8__(v16, 39))\n            + v263;\n        v265 = v274.m256i_i64[2] + v254;\n        v266 = (__ROR8__(v19, 14) ^ __ROR8__(v19, 18) ^ __ROR8__(v19, 41)) + (v258 ^ v19 & (v258 ^ v20));\n        v18 = v266 + v265 + v255;\n        v14 = (v259 & v15 | v16 & (v259 | v15))\n            + v266\n            + (__ROR8__(v15, 28) ^ __ROR8__(v15, 34) ^ __ROR8__(v15, 39))\n            + v265;\n        v267 = v274.m256i_i64[3] + v258;\n        v268 = (__ROR8__(v18, 14) ^ __ROR8__(v18, 18) ^ __ROR8__(v18, 41)) + (v20 ^ v18 & (v20 ^ v19));\n        v17 = v268 + v267 + v259;\n        v13 = (v16 & v14 | v15 & (v16 | v14))\n            + v268\n            + (__ROR8__(v14, 28) ^ __ROR8__(v14, 34) ^ __ROR8__(v14, 39))\n            + v267;\n        __asm\n        {\n          vmovdqu ymm4, ymm6\n          vmovdqu ymm5, ymm7\n        }\n        --v276;\n      }\n      while ( v276 );\n      v13 += *a10;\n      *a10 = v13;\n      v14 += a10[1];\n      a10[1] = v14;\n      v15 += a10[2];\n      a10[2] = v15;\n      v16 += a10[3];\n      a10[3] = v16;\n      v17 += a10[4];\n      a10[4] = v17;\n      v18 += a10[5];\n      a10[5] = v18;\n      v19 += a10[6];\n      a10[6] = v19;\n      v20 += a10[7];\n      a10[7] = v20;\n      _RDI += 128LL;\n    }\n    while ( v277 + 128 != a11 + (a12 >> 7 << 7) );\n  }\n  __asm { vzeroupper }\n}\n// 5EEC60: using guessed type void *off_5EEC60;\n\n"
    },
    {
        "Function": "project1_common_GetBlocksAmount",
        "Total XOR and shift operations": 10,
        "XOR operations": 0,
        "Shift operations": 10,
        "Operation percentage": "18.87%",
        "Function Body": "\n// project1/common.GetBlocksAmount\n__int64 __golang project1_common_GetBlocksAmount(__int64 a1, __int64 a2, __int64 a3)\n{\n  __int64 v4; // rax\n  __int64 v5; // rcx\n  __int64 v6; // rdx\n  __int64 v7; // rcx\n  __int64 v8; // rdx\n  double v9; // xmm0_8\n  __int64 v10; // rbx\n  __int64 v11; // rcx\n  double v12; // xmm1_8\n\n  if ( !a2 )\n  {\n    runtime_panicdivide();\n    JUMPOUT(0x4F1A9ELL);\n  }\n  if ( a2 == -1 )\n    v4 = -a1;\n  else\n    v4 = a1 / a2;\n  v5 = a1 - a3;\n  v6 = ((unsigned __int64)(v5 >> 63) >> 54) + v5;\n  v7 = v5 / 1024;\n  if ( v7 <= 1024 )\n    return v4;\n  v8 = ((unsigned __int64)(v6 >> 63) >> 54) + v7;\n  v9 = (double)(int)a2 * 0.0009765625 * 0.0009765625;\n  v10 = (unsigned int)(int)(0.2 * (double)(int)(v8 >> 10) / v9);\n  if ( v8 >> 10 >= 1024 )\n  {\n    v11 = (__int64)(((unsigned __int64)(v8 >> 63) >> 54) + (v8 >> 10)) >> 10;\n    if ( (unsigned __int64)(v11 - 101) >= 0x18F )\n    {\n      if ( v11 <= 500 )\n        v12 = 0.001;\n      else\n        v12 = 0.00005;\n    }\n    else\n    {\n      v12 = 0.00015;\n    }\n    return (unsigned int)(int)(v12 * (double)(int)v11 / (0.0009765625 * v9));\n  }\n  return v10;\n}\n// 4F1A9D: control flows out of bounds to 4F1A9E\n// 432740: using guessed type __int64 runtime_panicdivide(void);\n\n"
    },
    {
        "Function": "crypto_ed25519_internal_edwards25519_field_feSquare",
        "Total XOR and shift operations": 10,
        "XOR operations": 0,
        "Shift operations": 10,
        "Operation percentage": "17.54%",
        "Function Body": "\n// crypto/ed25519/internal/edwards25519/field.feSquare\n_QWORD *__golang crypto_ed25519_internal_edwards25519_field_feSquare(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        _QWORD *a10,\n        unsigned __int64 *a11)\n{\n  signed __int128 v11; // kr20_16\n  signed __int128 v12; // kr50_16\n  signed __int128 v13; // kr80_16\n  signed __int128 v14; // krB0_16\n  signed __int128 v15; // krE0_16\n  unsigned __int64 v16; // rsi\n  unsigned __int64 v17; // r8\n  unsigned __int64 v18; // r10\n  unsigned __int64 v19; // r12\n  unsigned __int64 v20; // r14\n  _QWORD *result; // rax\n\n  v11 = a11[3] * (unsigned __int128)(38 * a11[2])\n      + a11[4] * (unsigned __int128)(38 * a11[1])\n      + *a11 * (unsigned __int128)*a11;\n  v12 = a11[3] * (unsigned __int128)(19 * a11[3])\n      + a11[4] * (unsigned __int128)(38 * a11[2])\n      + a11[1] * (unsigned __int128)(2 * *a11);\n  v13 = a11[4] * (unsigned __int128)(38 * a11[3])\n      + a11[1] * (unsigned __int128)a11[1]\n      + a11[2] * (unsigned __int128)(2 * *a11);\n  v14 = a11[4] * (unsigned __int128)(19 * a11[4])\n      + a11[2] * (unsigned __int128)(2 * a11[1])\n      + a11[3] * (unsigned __int128)(2 * *a11);\n  v15 = a11[2] * (unsigned __int128)a11[2]\n      + a11[3] * (unsigned __int128)(2 * a11[1])\n      + a11[4] * (unsigned __int128)(2 * *a11);\n  v16 = 19 * (v15 >> 51) + (v11 & 0x7FFFFFFFFFFFFLL);\n  v17 = (v11 >> 51) + (v12 & 0x7FFFFFFFFFFFFLL);\n  v18 = (v12 >> 51) + (v13 & 0x7FFFFFFFFFFFFLL);\n  v19 = (v13 >> 51) + (v14 & 0x7FFFFFFFFFFFFLL);\n  v20 = (v14 >> 51) + (v15 & 0x7FFFFFFFFFFFFLL);\n  result = a10;\n  *a10 = 19 * (v20 >> 51) + (v16 & 0x7FFFFFFFFFFFFLL);\n  a10[1] = (v16 >> 51) + (v17 & 0x7FFFFFFFFFFFFLL);\n  a10[2] = (v17 >> 51) + (v18 & 0x7FFFFFFFFFFFFLL);\n  a10[3] = (v18 >> 51) + (v19 & 0x7FFFFFFFFFFFFLL);\n  a10[4] = (v19 >> 51) + (v20 & 0x7FFFFFFFFFFFFLL);\n  return result;\n}\n\n"
    },
    {
        "Function": "crypto_ed25519_internal_edwards25519_field_feMul",
        "Total XOR and shift operations": 10,
        "XOR operations": 0,
        "Shift operations": 10,
        "Operation percentage": "14.71%",
        "Function Body": "\n// crypto/ed25519/internal/edwards25519/field.feMul\n_QWORD *__golang crypto_ed25519_internal_edwards25519_field_feMul(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        _QWORD *a10,\n        unsigned __int64 *a11,\n        unsigned __int64 *a12)\n{\n  signed __int128 v12; // kr40_16\n  signed __int128 v13; // kr90_16\n  signed __int128 v14; // krE0_16\n  signed __int128 v15; // kr130_16\n  unsigned __int64 v16; // rdi\n  unsigned __int64 v17; // r9\n  unsigned __int64 v18; // r11\n  unsigned __int64 v19; // r13\n  unsigned __int64 v20; // r15\n  _QWORD *result; // rax\n\n  v12 = a12[2] * (unsigned __int128)(19 * a11[4])\n      + a12[3] * (unsigned __int128)(19 * a11[3])\n      + a12[4] * (unsigned __int128)(19 * a11[2])\n      + *a12 * (unsigned __int128)a11[1]\n      + a12[1] * (unsigned __int128)*a11;\n  v13 = a12[3] * (unsigned __int128)(19 * a11[4])\n      + a12[4] * (unsigned __int128)(19 * a11[3])\n      + *a12 * (unsigned __int128)a11[2]\n      + a12[1] * (unsigned __int128)a11[1]\n      + a12[2] * (unsigned __int128)*a11;\n  v14 = a12[4] * (unsigned __int128)(19 * a11[4])\n      + *a12 * (unsigned __int128)a11[3]\n      + a12[1] * (unsigned __int128)a11[2]\n      + a12[2] * (unsigned __int128)a11[1]\n      + a12[3] * (unsigned __int128)*a11;\n  v15 = *a12 * (unsigned __int128)a11[4]\n      + a12[1] * (unsigned __int128)a11[3]\n      + a12[2] * (unsigned __int128)a11[2]\n      + a12[3] * (unsigned __int128)a11[1]\n      + a12[4] * (unsigned __int128)*a11;\n  v16 = 19 * (v15 >> 51)\n      + ((a12[1] * 19 * a11[4] + a12[2] * 19 * a11[3] + a12[3] * 19 * a11[2] + a12[4] * 19 * a11[1] + *a12 * *a11) & 0x7FFFFFFFFFFFFLL);\n  v17 = ((__int128)(a12[1] * (unsigned __int128)(19 * a11[4])\n                  + a12[2] * (unsigned __int128)(19 * a11[3])\n                  + a12[3] * (unsigned __int128)(19 * a11[2])\n                  + a12[4] * (unsigned __int128)(19 * a11[1])\n                  + *a12 * (unsigned __int128)*a11) >> 51)\n      + (v12 & 0x7FFFFFFFFFFFFLL);\n  v18 = (v12 >> 51) + (v13 & 0x7FFFFFFFFFFFFLL);\n  v19 = (v13 >> 51) + (v14 & 0x7FFFFFFFFFFFFLL);\n  v20 = (v14 >> 51) + (v15 & 0x7FFFFFFFFFFFFLL);\n  result = a10;\n  *a10 = 19 * (v20 >> 51) + (v16 & 0x7FFFFFFFFFFFFLL);\n  a10[1] = (v16 >> 51) + (v17 & 0x7FFFFFFFFFFFFLL);\n  a10[2] = (v17 >> 51) + (v18 & 0x7FFFFFFFFFFFFLL);\n  a10[3] = (v18 >> 51) + (v19 & 0x7FFFFFFFFFFFFLL);\n  a10[4] = (v19 >> 51) + (v20 & 0x7FFFFFFFFFFFFLL);\n  return result;\n}\n\n"
    },
    {
        "Function": "cmpbody",
        "Total XOR and shift operations": 13,
        "XOR operations": 7,
        "Shift operations": 6,
        "Operation percentage": "9.03%",
        "Function Body": "\n// cmpbody\n__int64 __golang cmpbody(__int64 a1, signed __int64 a2, __int64 a3, const __m128i *_RDI, const __m128i *_RSI)\n{\n  signed __int64 v5; // rdx\n  unsigned __int64 v6; // r8\n  __int64 v7; // rax\n  unsigned __int64 v8; // rbx\n  unsigned __int64 v10; // rax\n  unsigned __int64 v11; // rcx\n  unsigned __int64 v12; // rax\n  char v13; // cl\n  unsigned __int64 v14; // rsi\n  unsigned __int64 v15; // rsi\n  unsigned __int64 v16; // rdi\n  unsigned __int64 v17; // rsi\n  unsigned __int64 v18; // rdi\n  unsigned __int64 v19; // rcx\n\n  if ( _RSI == _RDI )\n    return (a2 == v5) + 2LL * (a2 > v5) - 1;\n  v6 = v5;\n  if ( a2 < v5 )\n    v6 = a2;\n  if ( v6 < 8 )\n  {\n    v13 = -8 * v6;\n    if ( -8LL * v6 )\n    {\n      v14 = (unsigned __int8)_RSI > 0xF8u\n          ? *(unsigned __int64 *)((char *)&_RSI->m128i_i64[-1] + v6) >> v13\n          : _RSI->m128i_i64[0];\n      v15 = v14 << v13;\n      v16 = (unsigned __int8)_RDI > 0xF8u\n          ? *(unsigned __int64 *)((char *)&_RDI->m128i_i64[-1] + v6) >> v13\n          : _RDI->m128i_i64[0];\n      v17 = _byteswap_uint64(v15);\n      v18 = v17 ^ _byteswap_uint64(v16 << v13);\n      if ( v18 )\n      {\n        _BitScanReverse64(&v19, v18);\n        return 2 * ((v17 >> v19) & 1) - 1;\n      }\n    }\n    return (a2 == v5) + 2LL * (a2 > v5) - 1;\n  }\n  if ( v6 <= 0x3F )\n    goto LABEL_8;\n  if ( byte_64C043 == 1 )\n  {\n    do\n    {\n      __asm\n      {\n        vmovdqu ymm2, ymmword ptr [rsi]\n        vmovdqu ymm3, ymmword ptr [rdi]\n        vmovdqu ymm4, ymmword ptr [rsi+20h]\n        vmovdqu ymm5, ymmword ptr [rdi+20h]\n        vpcmpeqb ymm0, ymm3, ymm2\n        vpmovmskb eax, ymm0\n      }\n      v7 = (unsigned int)~_EAX;\n      if ( (_DWORD)v7 )\n      {\n        __asm { vzeroupper }\n        goto LABEL_14;\n      }\n      __asm\n      {\n        vpcmpeqb ymm6, ymm5, ymm4\n        vpmovmskb eax, ymm6\n      }\n      v7 = (unsigned int)~_EAX;\n      if ( (_DWORD)v7 )\n      {\n        __asm { vzeroupper }\nLABEL_12:\n        _RSI += 2;\n        _RDI += 2;\n        goto LABEL_14;\n      }\n      _RSI += 4;\n      _RDI += 4;\n      v6 -= 64LL;\n    }\n    while ( v6 >= 0x40 );\n    __asm { vzeroupper }\nLABEL_8:\n    while ( v6 > 0x10 )\n    {\n      v7 = (unsigned int)_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI), _mm_loadu_si128(_RSI))) ^ 0xFFFFLL;\n      if ( v7 )\n        goto LABEL_14;\n      ++_RSI;\n      ++_RDI;\n      v6 -= 16LL;\n    }\n    if ( v6 > 8 && (v10 = _RSI->m128i_i64[0], v11 = _RDI->m128i_i64[0], _RSI->m128i_i64[0] != _RDI->m128i_i64[0])\n      || (v10 = *(unsigned __int64 *)((char *)&_RSI->m128i_u64[-1] + v6),\n          v11 = *(unsigned __int64 *)((char *)&_RDI->m128i_u64[-1] + v6),\n          v10 != v11) )\n    {\n      v12 = _byteswap_uint64(v10);\n      _BitScanReverse64(&v11, v12 ^ _byteswap_uint64(v11));\n      return 2 * ((v12 >> v11) & 1) - 1;\n    }\n    return (a2 == v5) + 2LL * (a2 > v5) - 1;\n  }\n  while ( 1 )\n  {\n    v7 = (unsigned int)_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI), _mm_loadu_si128(_RSI))) ^ 0xFFFFLL;\n    if ( v7 )\n      break;\n    v7 = (unsigned int)_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI + 1), _mm_loadu_si128(_RSI + 1))) ^ 0xFFFFLL;\n    if ( v7 )\n    {\n      ++_RSI;\n      ++_RDI;\n      break;\n    }\n    v7 = (unsigned int)_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI + 2), _mm_loadu_si128(_RSI + 2))) ^ 0xFFFFLL;\n    if ( v7 )\n      goto LABEL_12;\n    v7 = (unsigned int)_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI + 3), _mm_loadu_si128(_RSI + 3))) ^ 0xFFFFLL;\n    if ( v7 )\n    {\n      _RSI += 3;\n      _RDI += 3;\n      break;\n    }\n    _RSI += 4;\n    _RDI += 4;\n    v6 -= 64LL;\n    if ( v6 <= 0x40 )\n      goto LABEL_8;\n  }\nLABEL_14:\n  _BitScanForward64(&v8, v7);\n  return 2LL * (_RSI->m128i_i8[v8] > (unsigned int)_RDI->m128i_i8[v8]) - 1;\n}\n// 4020CC: variable 'v5' is possibly undefined\n// 64C043: using guessed type char byte_64C043;\n\n"
    },
    {
        "Function": "countbody",
        "Total XOR and shift operations": 7,
        "XOR operations": 0,
        "Shift operations": 7,
        "Operation percentage": "7.95%",
        "Function Body": "\n// countbody\nconst __m128i *__golang countbody(\n        const __m128i *result,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        const __m128i *a5,\n        _QWORD *a6)\n{\n  __m128i v6; // xmm0\n  __m128i v7; // xmm0\n  __int64 v8; // r12\n  __int64 v10; // rbx\n  __int64 v11; // r10\n  __m128i v12; // xmm1\n\n  v6 = _mm_unpacklo_epi8((__m128i)(unsigned __int64)result, (__m128i)(unsigned __int64)result);\n  v7 = _mm_shuffle_epi32(_mm_unpacklo_epi8(v6, v6), 0);\n  if ( a2 < 16 )\n  {\n    if ( a2 )\n    {\n      result = a5 + 1;\n      if ( (((_WORD)a5 + 16) & 0xFF0) != 0 )\n      {\n        v11 = (1LL << a2) - 1;\n        v12 = _mm_loadu_si128(a5);\n      }\n      else\n      {\n        v11 = 0xFFFFLL >> (16 - (unsigned __int8)a2) << (16 - (unsigned __int8)a2);\n        v12 = _mm_loadu_si128((const __m128i *)((char *)a5 + a2 - 16));\n      }\n      *a6 = __popcnt(v11 & _mm_movemask_epi8(_mm_cmpeq_epi8(v12, v7)));\n    }\n    else\n    {\n      *a6 = 0LL;\n    }\n  }\n  else\n  {\n    v8 = 0LL;\n    _RDI = a5;\n    if ( (unsigned __int64)a2 > 0x20 && byte_64C043 == 1 )\n    {\n      _XMM0 = (unsigned __int64)result;\n      __asm { vpbroadcastb ymm1, xmm0 }\n      do\n      {\n        __asm\n        {\n          vmovdqu ymm2, ymmword ptr [rdi]\n          vpcmpeqb ymm3, ymm2, ymm1\n          vpmovmskb edx, ymm3\n        }\n        v8 += __popcnt(_EDX);\n        _RDI += 2;\n      }\n      while ( (__int64)_RDI <= (__int64)a5[-2].m128i_i64 + a2 );\n      _RDI = (__int64)a5[-2].m128i_i64 + a2;\n      __asm\n      {\n        vmovdqu ymm2, ymmword ptr [rdi]\n        vpcmpeqb ymm3, ymm2, ymm1\n        vpmovmskb edx, ymm3\n        vzeroupper\n      }\n      *a6 = __popcnt((0xFFFFFFFFLL >> (32 - ((unsigned __int8)a2 & 0x1Fu)) << (32 - ((unsigned __int8)a2 & 0x1Fu))) & _EDX)\n          + v8;\n    }\n    else\n    {\n      result = (const __m128i *)((char *)a5 + a2 - 16);\n      while ( _RDI <= result )\n        v8 += __popcnt(_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(_RDI++), v7)));\n      v10 = a2 & 0xF;\n      if ( v10 )\n        v8 += __popcnt((0xFFFFLL >> (16 - (unsigned __int8)v10) << (16 - (unsigned __int8)v10)) & _mm_movemask_epi8(_mm_cmpeq_epi8(_mm_loadu_si128(result), v7)));\n      *a6 = v8;\n    }\n  }\n  return result;\n}\n// 64C043: using guessed type char byte_64C043;\n\n"
    },
    {
        "Function": "math_big_shrVU",
        "Total XOR and shift operations": 3,
        "XOR operations": 0,
        "Shift operations": 3,
        "Operation percentage": "7.50%",
        "Function Body": "\n// math/big.shrVU\nvoid __golang math_big_shrVU(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        __int64 a10,\n        __int64 a11,\n        __int64 a12,\n        unsigned __int64 *a13,\n        __int64 a14,\n        __int64 a15,\n        __int64 a16)\n{\n  __int64 v16; // r11\n  unsigned __int64 v17; // rax\n  __int64 i; // rbx\n  unsigned __int64 v19; // rdx\n\n  v16 = a11 - 1;\n  if ( a11 >= 1 )\n  {\n    v17 = *a13;\n    for ( i = 0LL; i < v16; ++i )\n    {\n      v19 = v17;\n      v17 = a13[i + 1];\n      *(_QWORD *)(a10 + 8 * i) = (v19 >> a16) | (v17 << (64 - (unsigned __int8)a16));\n    }\n    *(_QWORD *)(a10 + 8 * v16) = v17 >> a16;\n  }\n}\n\n"
    },
    {
        "Function": "math_big_shlVU",
        "Total XOR and shift operations": 3,
        "XOR operations": 0,
        "Shift operations": 3,
        "Operation percentage": "6.98%",
        "Function Body": "\n// math/big.shlVU\nvoid __golang math_big_shlVU(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        _QWORD *a10,\n        __int64 a11,\n        __int64 a12,\n        __int64 a13,\n        __int64 a14,\n        __int64 a15,\n        __int64 a16)\n{\n  __int64 v16; // rbx\n  unsigned __int64 v17; // rax\n  unsigned __int64 v18; // rdx\n\n  v16 = a11 - 1;\n  if ( a11 >= 1 )\n  {\n    v17 = *(_QWORD *)(a13 + 8 * v16);\n    if ( v16 > 0 )\n    {\n      do\n      {\n        v18 = v17;\n        v17 = *(_QWORD *)(a13 + 8 * v16 - 8);\n        a10[v16] = (v18 << a16) | (v17 >> (64 - (unsigned __int8)a16));\n      }\n      while ( v16-- > 1 );\n    }\n    *a10 = v17 << a16;\n  }\n}\n\n"
    },
    {
        "Function": "math_big_addMulVVW",
        "Total XOR and shift operations": 7,
        "XOR operations": 0,
        "Shift operations": 7,
        "Operation percentage": "4.35%",
        "Function Body": "\n// math/big.addMulVVW\n// local variable allocation has failed, the output may be wrong!\n__int64 __golang math_big_addMulVVW(\n        __int64 a1,\n        __int64 a2,\n        __int64 a3,\n        __int64 a4,\n        __int64 a5,\n        __int64 a6,\n        __int64 a7,\n        __int64 a8,\n        __int64 a9,\n        _QWORD *a10,\n        __int64 a11,\n        __int64 a12,\n        unsigned __int64 *a13,\n        __int64 a14,\n        __int64 a15,\n        unsigned __int64 a16)\n{\n  signed __int64 v16; // rbx\n  unsigned __int64 v17; // rcx\n  unsigned __int64 v18; // rax\n  unsigned __int64 v19; // rcx\n  bool v21; // cf\n  unsigned __int64 *v23; // r8\n  signed __int64 v24; // rbx\n\n  if ( byte_64B7E9 == 1 )\n  {\n    _R10 = a10;\n    v23 = a13;\n    v24 = 0LL;\n    if ( (unsigned __int64)a11 >= 8 )\n    {\n      do\n      {\n        _R9 = 0LL;\n        _RDX = a16;\n        _RDI = (a16 * (unsigned __int128)*v23) >> 64;\n        _RSI = a16 * *v23;\n        __asm\n        {\n          adcx    rsi, rcx\n          adox    rsi, [r10]\n        }\n        *_R10 = _RSI;\n        _RAX = a16 * v23[1];\n        __asm\n        {\n          adcx    rax, rdi\n          adox    rax, [r10+8]\n        }\n        _R10[1] = _RAX;\n        _RDX = a16;\n        _RDI = (a16 * (unsigned __int128)v23[2]) >> 64;\n        _RSI = a16 * v23[2];\n        __asm\n        {\n          adcx    rsi, rcx\n          adox    rsi, [r10+10h]\n        }\n        _R10[2] = _RSI;\n        _RAX = a16 * v23[3];\n        __asm\n        {\n          adcx    rax, rdi\n          adox    rax, [r10+18h]\n        }\n        _R10[3] = _RAX;\n        _RDX = a16;\n        _RDI = (a16 * (unsigned __int128)v23[4]) >> 64;\n        _RSI = a16 * v23[4];\n        __asm\n        {\n          adcx    rsi, rcx\n          adox    rsi, [r10+20h]\n        }\n        _R10[4] = _RSI;\n        _RAX = a16 * v23[5];\n        __asm\n        {\n          adcx    rax, rdi\n          adox    rax, [r10+28h]\n        }\n        _R10[5] = _RAX;\n        *(unsigned __int128 *)((char *)&_RAX + 8) = a16;\n        _RDI = (a16 * (unsigned __int128)v23[6]) >> 64;\n        _RSI = a16 * v23[6];\n        __asm\n        {\n          adcx    rsi, rcx\n          adox    rsi, [r10+30h]\n        }\n        _R10[6] = _RSI;\n        *((_QWORD *)&_RDX + 1) = (a16 * (unsigned __int128)v23[7]) >> 64;\n        *(_QWORD *)&_RAX = a16 * v23[7];\n        __asm\n        {\n          adcx    rax, rdi\n          adox    rax, [r10+38h]\n        }\n        _R10[7] = _RAX;\n        __asm\n        {\n          adcx    rcx, r9\n          adox    rcx, r9\n        }\n        v23 += 8;\n        _R10 += 8;\n        v24 += 8LL;\n      }\n      while ( v24 < (__int64)(a11 & 0xFFFFFFFFFFFFFFF8LL) );\n      _R10 = a10;\n      v23 = a13;\n      if ( v24 >= a11 )\n        return _RAX;\n    }\n    else if ( a11 <= 0 )\n    {\n      return _RAX;\n    }\n    do\n    {\n      _R10[v24] += a16 * v23[v24];\n      ++v24;\n    }\n    while ( v24 < a11 );\n    return _RAX;\n  }\n  v16 = 0LL;\n  v17 = 0LL;\n  if ( (unsigned __int64)a11 >= 2 )\n  {\n    do\n    {\n      v18 = v17 + a10[v16] + a16 * a13[v16];\n      v19 = (v17 + (unsigned __int64)a10[v16] + a16 * (unsigned __int128)a13[v16]) >> 64;\n      a10[v16] = v18;\n      *(_QWORD *)&_RAX = v19 + a10[v16 + 1] + a16 * a13[v16 + 1];\n      v17 = (v19 + (unsigned __int64)a10[v16 + 1] + a16 * (unsigned __int128)a13[v16 + 1]) >> 64;\n      a10[v16 + 1] = _RAX;\n      v16 += 2LL;\n    }\n    while ( v16 < (__int64)(a11 & 0xFFFFFFFFFFFFFFFELL) );\n  }\n  while ( v16 < a11 )\n  {\n    _RAX = v17 + a16 * (unsigned __int128)a13[v16];\n    v21 = __CFADD__((_QWORD)_RAX, a10[v16]);\n    a10[v16] += _RAX;\n    v17 = v21 + *((_QWORD *)&_RAX + 1);\n    ++v16;\n  }\n  return _RAX;\n}\n// 4BACC0: variables would overlap: rax.16 and rdx.16\n// 64B7E9: using guessed type char byte_64B7E9;\n\n"
    },
    {
        "Function": "memeqbody",
        "Total XOR and shift operations": 3,
        "XOR operations": 0,
        "Shift operations": 3,
        "Operation percentage": "3.09%",
        "Function Body": "\n// memeqbody\nbool __golang memeqbody(__int64 a1, unsigned __int64 a2, __int64 a3, const __m128i *_RDI, const __m128i *_RSI)\n{\n  __m128i v5; // xmm0\n  __int64 v15; // rcx\n  __int64 v16; // rdx\n  bool v17; // zf\n  char v18; // cl\n  unsigned __int64 v19; // rsi\n  unsigned __int64 v20; // rdi\n\n  if ( a2 < 8 )\n  {\n    v17 = a2 == 0;\n    if ( a2 )\n    {\n      v18 = -8 * a2;\n      if ( (unsigned __int8)_RSI > 0xF8u )\n        v19 = *(unsigned __int64 *)((char *)&_RSI->m128i_i64[-1] + a2) >> v18;\n      else\n        v19 = _RSI->m128i_i64[0];\n      if ( (unsigned __int8)_RDI > 0xF8u )\n        v20 = *(unsigned __int64 *)((char *)&_RDI->m128i_i64[-1] + a2) >> v18;\n      else\n        v20 = _RDI->m128i_i64[0];\n      return (v20 - v19) << v18 == 0;\n    }\n    return v17;\n  }\n  else\n  {\n    if ( a2 < 0x40 )\n      goto LABEL_11;\n    if ( byte_64C043 == 1 )\n    {\n      while ( a2 >= 0x40 )\n      {\n        __asm\n        {\n          vmovdqu ymm0, ymmword ptr [rsi]\n          vmovdqu ymm1, ymmword ptr [rdi]\n          vmovdqu ymm2, ymmword ptr [rsi+20h]\n          vmovdqu ymm3, ymmword ptr [rdi+20h]\n          vpcmpeqb ymm4, ymm0, ymm1\n          vpcmpeqb ymm5, ymm3, ymm2\n          vpand   ymm6, ymm5, ymm4\n          vpmovmskb edx, ymm6\n        }\n        _RSI += 4;\n        _RDI += 4;\n        a2 -= 64LL;\n        if ( _EDX != -1 )\n        {\n          __asm { vzeroupper }\n          return 0;\n        }\n      }\n      __asm { vzeroupper }\nLABEL_11:\n      while ( a2 > 8 )\n      {\n        v15 = _RSI->m128i_i64[0];\n        v16 = _RDI->m128i_i64[0];\n        _RSI = (const __m128i *)((char *)_RSI + 8);\n        _RDI = (const __m128i *)((char *)_RDI + 8);\n        a2 -= 8LL;\n        if ( v15 != v16 )\n          return 0;\n      }\n      return *(__int64 *)((char *)&_RSI->m128i_i64[-1] + a2) == *(__int64 *)((char *)&_RDI->m128i_i64[-1] + a2);\n    }\n    else\n    {\n      do\n      {\n        if ( a2 < 0x40 )\n          goto LABEL_11;\n        v5 = _mm_and_si128(\n               _mm_and_si128(\n                 _mm_cmpeq_epi8(_mm_loadu_si128(_RSI), _mm_loadu_si128(_RDI)),\n                 _mm_cmpeq_epi8(_mm_loadu_si128(_RSI + 1), _mm_loadu_si128(_RDI + 1))),\n               _mm_and_si128(\n                 _mm_cmpeq_epi8(_mm_loadu_si128(_RSI + 2), _mm_loadu_si128(_RDI + 2)),\n                 _mm_cmpeq_epi8(_mm_loadu_si128(_RSI + 3), _mm_loadu_si128(_RDI + 3))));\n        _RSI += 4;\n        _RDI += 4;\n        a2 -= 64LL;\n      }\n      while ( _mm_movemask_epi8(v5) == 0xFFFF );\n      return 0;\n    }\n  }\n}\n// 64C043: using guessed type char byte_64C043;\n\n"
    }
]